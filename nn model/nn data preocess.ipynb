{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ed6b68-0736-48b5-9e48-dcb1b50cf323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "locale2ids = pickle.load(open('../data/local2ids.pkl', 'rb'))\n",
    "product_id2asin = pickle.load(open('../data/product_id2asin.pkl', 'rb'))\n",
    "product_asin2id = pickle.load(open('../data/product_asin2id.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a4fd1820-8341-4521-83d8-01fbc5b11f5e",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8505ab7-800b-447e-bf7a-b28c3c7dcf23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2930717/2930717 [00:18<00:00, 162672.77it/s]\n",
      "100%|██████████| 325635/325635 [00:02<00:00, 109978.28it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm \n",
    "from collections import Counter\n",
    "train_data = pd.read_pickle('../data/train_data_005.dataset')\n",
    "train_xgboost = train_data[::10]\n",
    "train_data = train_data.drop(index=train_xgboost.index).reset_index(drop=True)\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "data = []\n",
    "locales = []\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "    for _ in range(0,2):\n",
    "        data.append(row[:])\n",
    "        locales.append(new_locale[i])\n",
    "new_session = train_xgboost['session'].tolist()\n",
    "new_locale = train_xgboost['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    row = row[:-1]\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1363b6b1-b5f7-4ae7-bff6-4cb24a3a7fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331865/331865 [00:00<00:00, 456311.97it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_pickle('../data/train_data2_005.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    row = row[:-1]\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "382b94dc-7d0e-49d7-95c5-826d73ef45ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316971/316971 [00:00<00:00, 336086.73it/s]\n",
      "100%|██████████| 34688/34688 [00:00<00:00, 349729.50it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 326525.97it/s]\n",
      "100%|██████████| 34690/34690 [00:00<00:00, 355312.35it/s]\n",
      "100%|██████████| 60000/60000 [00:00<00:00, 403683.38it/s]\n",
      "100%|██████████| 52843/52843 [00:00<00:00, 413323.20it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_pickle('../data/test_data.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "    for _ in range(0,1):\n",
    "        if len(row)>=2:\n",
    "            data.append(row[:])\n",
    "            locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data2.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "    for _ in range(0,1):\n",
    "        if len(row)>=2:\n",
    "            data.append(row[:])\n",
    "            locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data_p2.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "    for _ in range(0,1):\n",
    "        if len(row)>=2:\n",
    "            data.append(row[:])\n",
    "            locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data2_p2.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "    for _ in range(0,1):\n",
    "        if len(row)>=2:\n",
    "            data.append(row[:])\n",
    "            locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data_p3.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "    for _ in range(0,1):\n",
    "        if len(row)>=2:\n",
    "            data.append(row[:])\n",
    "            locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data2_p3.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "    for _ in range(0,1):\n",
    "        if len(row)>=2:\n",
    "            data.append(row[:])\n",
    "            locales.append(new_locale[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc55c652-b5b8-46d3-9799-0f8608289cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316971/316971 [00:00<00:00, 413355.73it/s]\n",
      "100%|██████████| 34688/34688 [00:00<00:00, 536754.05it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 458994.12it/s]\n",
      "100%|██████████| 34690/34690 [00:00<00:00, 532746.05it/s]\n",
      "100%|██████████| 60000/60000 [00:00<00:00, 592629.27it/s]\n",
      "100%|██████████| 52843/52843 [00:00<00:00, 608013.05it/s]\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_pickle('../data/test_data.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data2.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data_p2.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data2_p2.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data_p3.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])\n",
    "train_data = pd.read_pickle('../data/test_data2_p3.dataset')\n",
    "new_session = train_data['session'].tolist()\n",
    "new_locale = train_data['locale'].tolist()\n",
    "for i in tqdm(range(len(new_session))):\n",
    "    row = []\n",
    "    row.append(new_session[i][0])\n",
    "    for item in new_session[i]:\n",
    "        if row[-1] == item:\n",
    "            continue\n",
    "        row.append(item)\n",
    "    for j in range((len(row)-3)):\n",
    "        data.append(row[:j+3])\n",
    "        locales.append(new_locale[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1582d53-4b90-4962-ac8f-bf2737b6d479",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>session</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[1017788, 925122, 1017788]</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[1017788, 925122, 1017788, 969194]</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[1017788, 925122, 1017788, 969194]</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1029808, 145265, 312739]</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1029808, 145265, 312739, 1029808]</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823904</th>\n",
       "      <td>[458320, 967193, 1398138, 1385511, 1405197, 14...</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823905</th>\n",
       "      <td>[458320, 967193, 1398138, 1385511, 1405197, 14...</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823906</th>\n",
       "      <td>[458320, 967193, 1398138, 1385511, 1405197, 14...</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823907</th>\n",
       "      <td>[379993, 1401965, 1388249]</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15823908</th>\n",
       "      <td>[379993, 1401965, 1388249, 1408268]</td>\n",
       "      <td>IT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15823909 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    session locale\n",
       "0                                [1017788, 925122, 1017788]     UK\n",
       "1                        [1017788, 925122, 1017788, 969194]     UK\n",
       "2                        [1017788, 925122, 1017788, 969194]     UK\n",
       "3                                 [1029808, 145265, 312739]     UK\n",
       "4                        [1029808, 145265, 312739, 1029808]     UK\n",
       "...                                                     ...    ...\n",
       "15823904  [458320, 967193, 1398138, 1385511, 1405197, 14...     IT\n",
       "15823905  [458320, 967193, 1398138, 1385511, 1405197, 14...     IT\n",
       "15823906  [458320, 967193, 1398138, 1385511, 1405197, 14...     IT\n",
       "15823907                         [379993, 1401965, 1388249]     IT\n",
       "15823908                [379993, 1401965, 1388249, 1408268]     IT\n",
       "\n",
       "[15823909 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame({'session': data, 'locale': locales})\n",
    "data_df.to_pickle('../data/train_data_plus_new.dataset')\n",
    "data_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
