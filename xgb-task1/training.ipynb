{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce79e081-30bf-46b4-ac4c-0424b4651a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f20d39bd-a1fd-4c7c-92d6-dca63d4b1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn import datasets\n",
    "cudf.set_option(\"default_integer_bitwidth\", 32)\n",
    "cudf.set_option(\"default_float_bitwidth\", 32)\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "767151bc-8abf-4587-ab4e-cf9849c055f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "            'user_user_count',\n",
    "            'user_item_count',\n",
    "            'remember_ratio',\n",
    "            'price',\n",
    "            'price_power_0_mean',\n",
    "            'price_power_1_mean',\n",
    "            'price_power_2_mean',\n",
    "            'price_power_3_mean',\n",
    "            'price_power_5_mean',\n",
    "            'nn_logits',\n",
    "            'jp',\n",
    "            'de',\n",
    "            'uk',\n",
    "           ]\n",
    "bad = [\n",
    "\n",
    "       72,73,74,75,76,77,#11 string\n",
    "       101,102,103,104,105,#15 brand\n",
    "       108,109,110,111,112,#16 author\n",
    "       115,116,117,118,119,#17 material\n",
    "       ]\n",
    "feature_list =  [[200,206]]\n",
    "for i in [1,2,3,4,5,6,7,9,10,11,12,13,16,15,17,18,19,21,22,23,27]:\n",
    "    feature_list.append([(i-1)*7+1,i*7])\n",
    "for i in feature_list:\n",
    "    for j in range(i[0],i[1]+1):\n",
    "        \n",
    "        if j in bad:\n",
    "            continue\n",
    "        features+=['ui_feat'+str(j)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cafde789-3f90-4518-8c55-f3616b1871a8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 15/23 [00:01<00:00,  9.82it/s]/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 74%|███████▍  | 17/23 [00:01<00:00,  9.33it/s]/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 78%|███████▊  | 18/23 [00:01<00:00,  9.11it/s]/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 83%|████████▎ | 19/23 [00:01<00:00,  9.00it/s]/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 87%|████████▋ | 20/23 [00:02<00:00,  8.24it/s]/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 91%|█████████▏| 21/23 [00:02<00:00,  8.14it/s]/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 96%|█████████▌| 22/23 [00:02<00:00,  8.10it/s]/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/583505980.py:34: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "100%|██████████| 23/23 [00:02<00:00,  9.57it/s]\n"
     ]
    }
   ],
   "source": [
    "valid = []\n",
    "data_set = ['JP']\n",
    "for locale__2 in data_set :#['DE','JP','UK']\n",
    "    temp_valid = pd.read_parquet('valid5_data_'+locale__2+'_0~0.pqt')\n",
    "    a = [0]*len(temp_valid)\n",
    "    b = [0]*len(temp_valid)\n",
    "    c = [0]*len(temp_valid)\n",
    "    if locale__2 == 'DE':\n",
    "        a = [1]*len(temp_valid)\n",
    "    if locale__2 == 'JP':\n",
    "        b = [1]*len(temp_valid)\n",
    "    if locale__2 == 'UK':\n",
    "        c = [1]*len(temp_valid)\n",
    "    temp_valid['de']=a\n",
    "    temp_valid['jp']=b\n",
    "    temp_valid['uk']=c\n",
    "    valid.append(temp_valid)\n",
    "    \n",
    "valid = pd.concat(valid).reset_index(drop=True)\n",
    "for row in tqdm(feature_list):\n",
    "    temp_df = []\n",
    "    for locale__2 in data_set :#['DE','JP','UK']\n",
    "        temp_df.append(pd.read_parquet('valid5_data_'+locale__2+'_'+str(row[0])+'~'+str(row[1])+'.pqt'))\n",
    "    temp_df = pd.concat(temp_df).reset_index(drop=True)\n",
    "    \n",
    "    for j in range(row[0],row[1]+1):\n",
    "        if j in bad:\n",
    "            continue\n",
    "        valid['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
    "    del temp_df\n",
    "\n",
    "X_valid = valid[features]\n",
    "y_valid = valid['target']\n",
    "dvalid = xgb.DMatrix(cudf.DataFrame(X_valid),cudf.DataFrame( y_valid), group=[250] * (len(X_valid)//250) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa2173b9-950a-44dc-a14f-4ae4b3f8442e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e507d521-47e1-49ad-9f33-8cacc5a371ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "# GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43966f3e-e7b4-46e4-b488-a9cf5ff506ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IterLoadForDMatrix(xgb.core.DataIter):\n",
    "    def __init__(self, df=None, features=None, batch_size=1024*256):\n",
    "        self.features = features\n",
    "        self.target = 'target'\n",
    "        self.df = df\n",
    "        self.it = 0 # set iterator to 0\n",
    "        self.batch_size = batch_size\n",
    "        self.batches = int( np.ceil( len(df) / self.batch_size ) )\n",
    "        super().__init__()\n",
    "\n",
    "    def reset(self):\n",
    "        '''Reset the iterator'''\n",
    "        self.it = 0\n",
    "\n",
    "    def next(self, input_data):\n",
    "        '''Yield next batch of data.'''\n",
    "        if self.it == self.batches:\n",
    "            return 0 # Return 0 when there's no more batch.\n",
    "        \n",
    "        a = self.it * self.batch_size\n",
    "        b = min( (self.it + 1) * self.batch_size, len(self.df) )\n",
    "        dt = cudf.DataFrame(self.df.iloc[a:b])\n",
    "        input_data(data=dt[self.features], label=dt[self.target]) #, weight=dt['weight'])\n",
    "        self.it += 1\n",
    "        return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "de484f54-409c-4eb3-b9d0-7c9b3955b0b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# mrr100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fd92409-ea0d-46e9-8977-bdd9ad16715b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "def mrr_at_k(predt: np.ndarray, dtrain: xgb.DMatrix, k: int) -> Tuple[str, float]:\n",
    "    y = dtrain.get_label()\n",
    "    group_sizes = dtrain.get_group()\n",
    "    assert len(y) == sum(group_sizes), \"group sizes must match data length\"\n",
    "    assert len(predt) == sum(group_sizes), \"group sizes must match prediction length\"\n",
    "    mrr_sum = 0.\n",
    "    mrr_count = 0\n",
    "    start_idx = 0\n",
    "    for i, group_size in enumerate(group_sizes):\n",
    "        end_idx = start_idx + group_size\n",
    "        group_y = y[start_idx:end_idx]\n",
    "        group_predt = predt[start_idx:end_idx]\n",
    "        order = np.argsort(-group_predt)\n",
    "        ranks = np.zeros_like(order)\n",
    "        ranks[order] = np.arange(len(group_predt))\n",
    "        reciprocal_ranks = 1. / (ranks + 1.)\n",
    "        if (group_y > 0).sum() == 0:\n",
    "            mrr = 0.\n",
    "        else:\n",
    "            mrr = np.mean(reciprocal_ranks[group_y > 0])\n",
    "        mrr_sum += mrr\n",
    "        mrr_count += 1\n",
    "        start_idx = end_idx\n",
    "    mrr_at_k = mrr_sum / mrr_count if mrr_count > 0 else 0.\n",
    "    return f\"mrr@{k}\", mrr_at_k"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17bedf98-c20b-43e9-a1df-064d5b6d7dda",
   "metadata": {
    "tags": []
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "befeaf72-0ee2-45d6-a280-169bf33ec153",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 15/23 [02:08<01:25, 10.63s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 70%|██████▉   | 16/23 [02:21<01:20, 11.44s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 74%|███████▍  | 17/23 [02:29<01:02, 10.47s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 78%|███████▊  | 18/23 [02:43<00:57, 11.52s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 83%|████████▎ | 19/23 [02:50<00:40, 10.15s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 87%|████████▋ | 20/23 [02:57<00:27,  9.09s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 91%|█████████▏| 21/23 [03:02<00:16,  8.01s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      " 96%|█████████▌| 22/23 [03:13<00:08,  8.86s/it]/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "/tmp/ipykernel_1996358/1071215783.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
      "100%|██████████| 23/23 [03:29<00:00,  9.11s/it]\n",
      "/home/sjliu/anaconda3/envs/rapids-23.02/lib/python3.10/site-packages/xgboost/core.py:1461: FutureWarning: Please use `QuantileDMatrix` instead.\n",
      "  warnings.warn(\"Please use `QuantileDMatrix` instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "data_set = ['DE','JP','UK']\n",
    "for locale__2 in data_set :\n",
    "    temp_train = pd.read_parquet('train5_data_'+locale__2+'_0~0.pqt')\n",
    "    a = [0]*len(temp_train)\n",
    "    b = [0]*len(temp_train)\n",
    "    c = [0]*len(temp_train)\n",
    "    if locale__2 == 'DE':\n",
    "        a = [1]*len(temp_train)\n",
    "    if locale__2 == 'JP':\n",
    "        b = [1]*len(temp_train)\n",
    "    if locale__2 == 'UK':\n",
    "        c = [1]*len(temp_train)\n",
    "    temp_train['de']=a\n",
    "    temp_train['jp']=b\n",
    "    temp_train['uk']=c\n",
    "    train.append(temp_train)\n",
    "    \n",
    "train = pd.concat(train).reset_index(drop=True)\n",
    "for row in tqdm(feature_list):\n",
    "    temp_df = []\n",
    "    for locale__2 in data_set :\n",
    "        temp_df.append(pd.read_parquet('train5_data_'+locale__2+'_'+str(row[0])+'~'+str(row[1])+'.pqt'))\n",
    "    temp_df = pd.concat(temp_df).reset_index(drop=True)\n",
    "    \n",
    "    for j in range(row[0],row[1]+1):\n",
    "        if j in bad:\n",
    "            continue\n",
    "        train['ui_feat'+str(j)] = temp_df['ui_feat'+str(j)]\n",
    "    del temp_df\n",
    "\n",
    "Xy_train = IterLoadForDMatrix(train, features)\n",
    "dtrain = xgb.DeviceQuantileDMatrix(Xy_train, max_bin=256)\n",
    "dtrain.set_group([250] * (len(train)//250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c93947-c7f4-4f99-b14d-c58c3d85bc51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3512649190064116"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = valid['target'].tolist()\n",
    "rank = valid['rank'].tolist()\n",
    "mrr = 0\n",
    "for i in range(len(rank)//250):\n",
    "    for j in range(100):\n",
    "        if target[i*250+j] == 1:\n",
    "            mrr+=1/(1+j)\n",
    "mrr/len(rank)*250\n",
    "#0.3539636726293081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfdd2400-a400-4276-aa69-dbeb1e8135c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################################\n",
      "max_depth:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sjliu/anaconda3/envs/rapids-23.02/lib/python3.10/site-packages/xgboost/training.py:39: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-map:0.59339\ttrain-mrr@100:0.31731\tvalid-map:0.61166\tvalid-mrr@100:0.34936\n",
      "[0]\ttrain-map:0.59339\tvalid-map:0.61166\n",
      "[100]\ttrain-map:0.63055\tvalid-map:0.64886\n",
      "[200]\ttrain-map:0.63928\tvalid-map:0.66077\n",
      "[300]\ttrain-map:0.64153\tvalid-map:0.66348\n",
      "[400]\ttrain-map:0.64300\tvalid-map:0.66349\n",
      "[500]\ttrain-map:0.64391\tvalid-map:0.66444\n",
      "[600]\ttrain-map:0.64483\tvalid-map:0.66471\n",
      "[700]\ttrain-map:0.64533\tvalid-map:0.66451\n",
      "[800]\ttrain-map:0.64588\tvalid-map:0.66436\n",
      "[900]\ttrain-map:0.64640\tvalid-map:0.66504\n",
      "[1000]\ttrain-map:0.64683\tvalid-map:0.66524\n",
      "[1100]\ttrain-map:0.64725\tvalid-map:0.66460\n",
      "[1136]\ttrain-map:0.64741\tvalid-map:0.66459\n",
      "[0]\ttrain-map:0.64741\ttrain-mrr@100:0.37666\tvalid-map:0.66455\tvalid-mrr@100:0.40811\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for hp in [1]:\n",
    "    print(\"############################################\")\n",
    "    print('max_depth: ',hp)\n",
    "    xgb_parms = { \n",
    "            'max_depth':4, \n",
    "            'learning_rate':0.07, \n",
    "            'subsample':0.5,\n",
    "            'colsample_bytree':0.6,\n",
    "            'reg_lambda':0.05,\n",
    "            'eval_metric':'map',\n",
    "            'objective':'binary:logistic',\n",
    "            # 'objective':'rank:pairwise',\n",
    "            'scale_pos_weight':8,\n",
    "            'tree_method':'gpu_hist',\n",
    "            'predictor':'gpu_predictor',\n",
    "            'num_feature':len(features),\n",
    "            'random_state':42 \n",
    "        }\n",
    "    model = None\n",
    "\n",
    "    for epoch in range(1):\n",
    "        for i in range(0,1):\n",
    "            \n",
    "            _ = xgb.train(xgb_parms, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                feval=lambda predt, dtrain: mrr_at_k(predt, dtrain, k=100),\n",
    "                num_boost_round=1,\n",
    "                early_stopping_rounds=0,\n",
    "                verbose_eval=1,\n",
    "                xgb_model=model)\n",
    "            model = xgb.train(xgb_parms, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                num_boost_round=3500,\n",
    "                early_stopping_rounds=200,\n",
    "                verbose_eval=100,\n",
    "                xgb_model=model)\n",
    "            _ = xgb.train(xgb_parms, \n",
    "                dtrain=dtrain,\n",
    "                evals=[(dtrain,'train'),(dvalid,'valid')],\n",
    "                feval=lambda predt, dtrain: mrr_at_k(predt, dtrain, k=100),\n",
    "                num_boost_round=1,\n",
    "                early_stopping_rounds=0,\n",
    "                verbose_eval=1,\n",
    "                xgb_model=model)#0.36880"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "812d7bfb-ab93-46dd-9f32-ad167f0918ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15085.6591796875 \t ui_feat187\n",
      "14723.5029296875 \t ui_feat161\n",
      "8814.6640625 \t ui_feat6\n",
      "8428.3837890625 \t nn_logits\n",
      "6728.07470703125 \t ui_feat185\n",
      "4011.457275390625 \t ui_feat67\n",
      "3109.309326171875 \t ui_feat188\n",
      "3011.5244140625 \t ui_feat90\n",
      "2917.0830078125 \t ui_feat70\n",
      "2397.842529296875 \t ui_feat66\n",
      "2325.269775390625 \t ui_feat65\n",
      "2251.427734375 \t ui_feat91\n",
      "2100.471435546875 \t ui_feat123\n",
      "2005.0399169921875 \t ui_feat88\n",
      "1900.083251953125 \t ui_feat48\n",
      "1760.7314453125 \t ui_feat68\n",
      "1518.6558837890625 \t ui_feat121\n",
      "1478.08056640625 \t ui_feat7\n",
      "1240.62060546875 \t ui_feat125\n",
      "1041.3966064453125 \t ui_feat84\n",
      "1034.052001953125 \t ui_feat85\n",
      "1028.663818359375 \t ui_feat122\n",
      "1022.7891845703125 \t ui_feat143\n",
      "864.6069946289062 \t ui_feat186\n",
      "660.3601684570312 \t ui_feat133\n",
      "653.9398803710938 \t ui_feat36\n",
      "643.6629638671875 \t ui_feat41\n",
      "623.8285522460938 \t ui_feat69\n",
      "615.6431884765625 \t ui_feat42\n",
      "597.433837890625 \t ui_feat46\n",
      "558.5595703125 \t ui_feat200\n",
      "526.03759765625 \t ui_feat99\n",
      "524.6970825195312 \t ui_feat146\n",
      "518.7783813476562 \t ui_feat206\n",
      "516.4942016601562 \t ui_feat205\n",
      "476.5202941894531 \t ui_feat89\n",
      "472.6624450683594 \t ui_feat132\n",
      "427.45599365234375 \t ui_feat150\n",
      "360.2101745605469 \t ui_feat83\n",
      "351.509765625 \t ui_feat151\n",
      "335.5111389160156 \t ui_feat49\n",
      "309.00238037109375 \t ui_feat152\n",
      "306.5184631347656 \t ui_feat145\n",
      "305.8580627441406 \t ui_feat78\n",
      "304.8687744140625 \t ui_feat144\n",
      "278.5163879394531 \t remember_ratio\n",
      "263.0559997558594 \t ui_feat28\n",
      "239.56187438964844 \t jp\n",
      "236.41099548339844 \t ui_feat147\n",
      "226.54908752441406 \t ui_feat100\n",
      "224.98558044433594 \t user_user_count\n",
      "220.51113891601562 \t ui_feat131\n",
      "218.01382446289062 \t ui_feat64\n",
      "205.03878784179688 \t ui_feat153\n",
      "197.34056091308594 \t ui_feat27\n",
      "194.8120880126953 \t ui_feat203\n",
      "193.99887084960938 \t ui_feat201\n",
      "178.73367309570312 \t ui_feat127\n",
      "176.02371215820312 \t ui_feat202\n",
      "172.36614990234375 \t ui_feat60\n",
      "168.76675415039062 \t ui_feat128\n",
      "163.60137939453125 \t ui_feat26\n",
      "161.21170043945312 \t ui_feat63\n",
      "157.2702178955078 \t ui_feat25\n",
      "153.50668334960938 \t price\n",
      "152.75636291503906 \t ui_feat20\n",
      "145.76731872558594 \t ui_feat33\n",
      "143.17041015625 \t ui_feat71\n",
      "141.77049255371094 \t user_item_count\n",
      "134.64312744140625 \t ui_feat29\n",
      "133.8800506591797 \t ui_feat129\n",
      "133.30625915527344 \t price_power_1_mean\n",
      "131.87164306640625 \t ui_feat13\n",
      "131.74046325683594 \t ui_feat1\n",
      "131.52401733398438 \t ui_feat120\n",
      "131.23098754882812 \t ui_feat106\n",
      "120.32491302490234 \t ui_feat61\n",
      "120.20660400390625 \t ui_feat124\n",
      "119.39286804199219 \t ui_feat126\n",
      "119.34786224365234 \t ui_feat34\n",
      "118.95989990234375 \t ui_feat204\n",
      "114.96707153320312 \t ui_feat19\n",
      "113.77095031738281 \t ui_feat113\n",
      "111.85868072509766 \t price_power_3_mean\n",
      "111.46371459960938 \t ui_feat86\n",
      "108.95205688476562 \t ui_feat82\n",
      "106.82322692871094 \t ui_feat22\n",
      "106.08795928955078 \t ui_feat4\n",
      "105.97371673583984 \t ui_feat12\n",
      "105.06748962402344 \t ui_feat14\n",
      "103.94790649414062 \t ui_feat158\n",
      "103.67304229736328 \t ui_feat32\n",
      "103.5221176147461 \t uk\n",
      "101.97090911865234 \t ui_feat114\n",
      "101.45589447021484 \t ui_feat80\n",
      "101.1255874633789 \t ui_feat79\n",
      "100.3988037109375 \t ui_feat8\n",
      "100.21365356445312 \t ui_feat183\n",
      "99.4677505493164 \t ui_feat3\n",
      "97.9597396850586 \t ui_feat15\n",
      "97.64763641357422 \t price_power_2_mean\n",
      "97.64018249511719 \t ui_feat81\n",
      "96.11736297607422 \t ui_feat141\n",
      "95.51068115234375 \t price_power_0_mean\n",
      "95.36235809326172 \t ui_feat35\n",
      "94.26990509033203 \t ui_feat11\n",
      "93.37677001953125 \t ui_feat9\n",
      "92.83319854736328 \t ui_feat47\n",
      "92.32347869873047 \t ui_feat44\n",
      "91.91986083984375 \t ui_feat23\n",
      "91.74461364746094 \t ui_feat148\n",
      "91.57405853271484 \t ui_feat5\n",
      "91.20492553710938 \t ui_feat39\n",
      "90.66908264160156 \t ui_feat43\n",
      "88.63990783691406 \t ui_feat2\n",
      "88.3193130493164 \t price_power_5_mean\n",
      "87.68214416503906 \t ui_feat37\n",
      "85.59346771240234 \t ui_feat40\n",
      "84.58946990966797 \t ui_feat21\n",
      "84.49461364746094 \t ui_feat10\n",
      "83.8597183227539 \t de\n",
      "82.42745208740234 \t ui_feat30\n",
      "81.87959289550781 \t ui_feat142\n",
      "81.82595825195312 \t ui_feat18\n",
      "80.98444366455078 \t ui_feat62\n",
      "80.13525390625 \t ui_feat57\n",
      "80.12785339355469 \t ui_feat159\n",
      "79.85903930664062 \t ui_feat154\n",
      "79.74137878417969 \t ui_feat59\n",
      "78.97887420654297 \t ui_feat87\n",
      "77.7369155883789 \t ui_feat31\n",
      "76.76114654541016 \t ui_feat160\n",
      "74.63983917236328 \t ui_feat58\n",
      "74.48835754394531 \t ui_feat157\n",
      "73.4358139038086 \t ui_feat130\n",
      "72.70082092285156 \t ui_feat17\n",
      "72.4739990234375 \t ui_feat16\n",
      "70.68888854980469 \t ui_feat107\n",
      "70.60457611083984 \t ui_feat38\n",
      "68.81364440917969 \t ui_feat149\n",
      "68.69170379638672 \t ui_feat45\n",
      "66.72110748291016 \t ui_feat184\n",
      "64.28878784179688 \t ui_feat156\n",
      "57.90005111694336 \t ui_feat155\n",
      "56.92591094970703 \t ui_feat24\n"
     ]
    }
   ],
   "source": [
    "a = features#list(model.get_score(importance_type='gain').keys())\n",
    "b = list(model.get_score(importance_type='gain').values())\n",
    "Z = zip(b, a)  # 对AB进行封装，把频率放在前面\n",
    "Z = sorted(Z, reverse=True)  # 进行逆序排列\n",
    "b, a = zip(*Z)  # 进行解压，其中的AB已经按照频率排好\n",
    "for i,j in zip(b, a):\n",
    "    print(i,'\\t',j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13117710-ed3c-4d86-9c98-cecfa8d4bdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    " \n",
    "pid = os.getpid()\n",
    "!kill -9 $pid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b515f087-536d-43f0-b248-31af49698612",
   "metadata": {
    "tags": []
   },
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f44a63a-e392-46a2-9f90-66bfb8c7460d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test\n",
    "xgb_parms = { \n",
    "            'max_depth':4, \n",
    "            'learning_rate':0.07, \n",
    "            'subsample':0.5,\n",
    "            'colsample_bytree':0.6,\n",
    "            'reg_lambda':0.05,\n",
    "            'eval_metric':'map',\n",
    "            'objective':'binary:logistic',\n",
    "            'scale_pos_weight':8,\n",
    "            'tree_method':'gpu_hist',\n",
    "            'predictor':'gpu_predictor',\n",
    "            'random_state':42\n",
    "        }\n",
    "model = xgb.Booster()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dc8b0e11-c54a-461a-82cf-b72259ec8c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [03:51<00:00, 77.22s/it]\n"
     ]
    }
   ],
   "source": [
    "recall1 = []\n",
    "le = 0\n",
    "for locale__ in tqdm(['DE','JP','UK']):\n",
    "\n",
    "    model = xgb.Booster()\n",
    "    model.load_model(f'XGB_locale_sum3.xgb')\n",
    "    data = cudf.read_parquet('test5_data_'+locale__+'_0~0.pqt')\n",
    "    a = [0]*len(data)\n",
    "    b = [0]*len(data)\n",
    "    c = [0]*len(data)\n",
    "    if locale__ == 'DE':\n",
    "        a = [1]*len(data)\n",
    "    if locale__ == 'JP':\n",
    "        b = [1]*len(data)\n",
    "    if locale__ == 'UK':\n",
    "        c = [1]*len(data)\n",
    "    data['de']=cudf.DataFrame(a)\n",
    "    data['jp']=cudf.DataFrame(b)\n",
    "    data['uk']=cudf.DataFrame(c)\n",
    "    if len(data)==0:\n",
    "        break\n",
    "    for row in feature_list:\n",
    "        if row[0]==200:\n",
    "            temp_df = cudf.read_parquet('test5_data_'+locale__+'_'+str(row[0])+'~'+str(row[1])+'.pqt')\n",
    "        else:\n",
    "            temp_df = cudf.read_parquet('test5_data_'+locale__+'_'+str(row[0])+'~'+str(row[1])+'.pqt')\n",
    "        for j in range(row[0],row[1]+1):\n",
    "            if j in bad:\n",
    "                continue\n",
    "            data['ui_feat'+str(j)] = cudf.DataFrame(temp_df['ui_feat'+str(j)])\n",
    "    le+=len(data)\n",
    "\n",
    "    dtest = xgb.DMatrix(data=cudf.DataFrame(data[features]))\n",
    "    preds = model.predict(dtest)\n",
    "    preds = np.array(preds)*np.log(np.e+10*data['sum_pop_score'].to_pandas())\n",
    "    del dtest\n",
    "    predictions = cudf.DataFrame(data[['user', 'item']])\n",
    "    predictions['pred'] = preds\n",
    "    predictions = predictions.sort_values(['user', 'pred'], ascending=[True, False]).reset_index(drop=True)\n",
    "    predictions['rk'] = predictions.groupby('user').item.cumcount().astype('int16')\n",
    "    predictions = predictions.loc[predictions.rk < 150]\n",
    "    predictions = predictions.to_pandas()\n",
    "    predictions = predictions.groupby('user').item.apply(list).to_frame().reset_index()\n",
    "    predictions.columns = ['test_user', 'next_item_prediction']\n",
    "    recall1 += predictions['next_item_prediction'].tolist()\n",
    "    del predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36358d70-63b8-41c7-874b-eb9222b8b241",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 直接生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b556eafc-7eb7-4d8f-893c-eba296b65d27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B084CB7GX9, B004P4QFJM, B08HQWQ1SK, B004P4OF1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B00V6FIFZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08GY1QYXP, B08GYG6T12, B08GY61ZZN, B09X7C7NC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...\n",
       "1          DE  [B084CB7GX9, B004P4QFJM, B08HQWQ1SK, B004P4OF1...\n",
       "2          DE  [B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...\n",
       "3          DE  [B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...\n",
       "4          DE  [B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B00V6FIFZ...\n",
       "316968     UK  [B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...\n",
       "316969     UK  [B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...\n",
       "316970     UK  [B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...\n",
       "316971     UK  [B08GY1QYXP, B08GYG6T12, B08GY61ZZN, B09X7C7NC...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "product_id2asin = pickle.load(open('../data/product_id2asin.pkl', 'rb'))\n",
    "locale = pd.read_pickle('../data/test_data_p2.dataset').reset_index(drop=True)['locale']\n",
    "result = [[product_id2asin[x] for x in row[:100]] for row in recall1]\n",
    "result = pd.DataFrame({'locale':locale,'next_item_prediction':result})\n",
    "result.to_parquet( 'submission_locale.parquet', engine='pyarrow')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "39fceec0-0ca8-4535-b39e-d1b8e7616d77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B084CB7GX9, B004P4QFJM, B08HQWQ1SK, B004P4OF1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B00V6FIFZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08GY1QYXP, B08GYG6T12, B08GY61ZZN, B09X7C7NC...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...\n",
       "1          DE  [B084CB7GX9, B004P4QFJM, B08HQWQ1SK, B004P4OF1...\n",
       "2          DE  [B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...\n",
       "3          DE  [B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...\n",
       "4          DE  [B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B00V6FIFZ...\n",
       "316968     UK  [B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...\n",
       "316969     UK  [B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...\n",
       "316970     UK  [B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...\n",
       "316971     UK  [B08GY1QYXP, B08GYG6T12, B08GY61ZZN, B09X7C7NC...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "product_id2asin = pickle.load(open('../data/product_id2asin.pkl', 'rb'))\n",
    "locale = pd.read_pickle('../data/test_data_p2.dataset').reset_index(drop=True)['locale']\n",
    "result = [[product_id2asin[x] for x in row[:100]] for row in recall1]\n",
    "result = pd.DataFrame({'locale':locale,'next_item_prediction':result})\n",
    "result.to_parquet( 'submission_locale.parquet', engine='pyarrow')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c4d79bc-70f3-496a-91d7-601c529c578b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B084CB7GX9, B08HQWQ1SK, B004P4QFJM, B004P4OF1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B016RAAUE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08GY1QYXP, B08GY61ZZN, B08GYG6T12, B07H9DVLB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...\n",
       "1          DE  [B084CB7GX9, B08HQWQ1SK, B004P4QFJM, B004P4OF1...\n",
       "2          DE  [B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...\n",
       "3          DE  [B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...\n",
       "4          DE  [B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B016RAAUE...\n",
       "316968     UK  [B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...\n",
       "316969     UK  [B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...\n",
       "316970     UK  [B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...\n",
       "316971     UK  [B08GY1QYXP, B08GY61ZZN, B08GYG6T12, B07H9DVLB...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "product_id2asin = pickle.load(open('../data/product_id2asin.pkl', 'rb'))\n",
    "locale = pd.read_pickle('../data/test_data_p2.dataset').reset_index(drop=True)['locale']\n",
    "result = [[product_id2asin[x] for x in row[:100]] for row in recall1]\n",
    "result = pd.DataFrame({'locale':locale,'next_item_prediction':result})\n",
    "result.to_parquet( 'submission_locale_sum.parquet', engine='pyarrow')\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54a6ff9-d2fb-4c62-97d9-6f9c2cbbe23b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B084CB7GX9, B004P4QFJM, B08HQWQ1SK, B004P4OF1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DE</td>\n",
       "      <td>[B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316967</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B00V6FIFZ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316968</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316969</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316970</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316971</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08GY1QYXP, B08GYG6T12, B08GY61ZZN, B01LORO7Z...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>316972 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       locale                               next_item_prediction\n",
       "0          DE  [B091CK241X, B07SDFLVKD, B0BGC82WVW, B093X59B3...\n",
       "1          DE  [B084CB7GX9, B004P4QFJM, B08HQWQ1SK, B004P4OF1...\n",
       "2          DE  [B09Z4PZQBF, B08LLF9M11, B09GPJ15GS, B09KBCTXF...\n",
       "3          DE  [B07Y1KLF25, B07QQZD49D, B07T5XJW9G, B07SZHKGZ...\n",
       "4          DE  [B0B2JY9THB, B08SXLWXH9, B08YK8FQJ8, B08P94RML...\n",
       "...       ...                                                ...\n",
       "316967     UK  [B07GKP2LCF, B07GKYSHB4, B006DDGCI2, B00V6FIFZ...\n",
       "316968     UK  [B00M35Y326, B08NPR9NFX, B086HRFHQH, B085C7TCT...\n",
       "316969     UK  [B08VDHH6QF, B08VD5DC5L, B07QK2SPP7, B08VDGZ8K...\n",
       "316970     UK  [B089CZWB4C, B09WCQYGX8, B08W2JJZBM, B089CJ7P7...\n",
       "316971     UK  [B08GY1QYXP, B08GYG6T12, B08GY61ZZN, B01LORO7Z...\n",
       "\n",
       "[316972 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "45fa5bd8-2a87-42d3-b79b-41e5fc231842",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 联合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b23d6ae4-ce2f-44a7-9222-1b88ae88b196",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = pd.read_parquet( 'submission_locale_sum.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86ac756f-22b6-43b7-899b-6de5e41d66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_parquet( '../task1/submission_locale_sum.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0cf05e-9ad6-4589-86ed-f4c60098c2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall1 = b['next_item_prediction'].tolist()\n",
    "recall2 = a['next_item_prediction'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146cdf85-0853-4ea2-9646-55be28407d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316972/316972 [01:22<00:00, 3856.43it/s]\n"
     ]
    }
   ],
   "source": [
    "recall = []\n",
    "for i in tqdm(range(len(recall1))):\n",
    "    output = []\n",
    "    for j in range(100):\n",
    "        if str(recall1[i][j]) not in output:\n",
    "            output.append(recall1[i][j])\n",
    "        if str(recall2[i][j]) not in output:\n",
    "            output.append(recall2[i][j])\n",
    "    recall.append(output[:100])\n",
    "a['next_item_prediction'] = recall\n",
    "a.to_parquet( 'submission_locale_xgb.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa0ada5-9187-4840-8c9e-305fcf626c18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudf",
   "language": "python",
   "name": "rapids-23.02"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
