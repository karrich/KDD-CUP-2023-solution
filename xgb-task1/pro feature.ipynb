{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f765bd-cf5a-499d-979e-91ff7fd919e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from math import sqrt\n",
    "from tqdm import tqdm\n",
    "pro_df = pd.read_csv(\"../data/products_train.csv\", sep=',')\n",
    "product_asin2id = pickle.load(open('../data/product_asin2id.pkl', 'rb'))\n",
    "locale2ids = pickle.load(open('../data/local2ids.pkl', 'rb'))\n",
    "item = pro_df['id']\n",
    "item = [product_asin2id[x] for x in item]\n",
    "pro_df['item'] = item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312b9ac-7299-4f2b-8549-e7f641bfe246",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When generating the training and validation sets,\n",
    "you need to select 'train', \n",
    "and when generating the test set, \n",
    "you need to select 'test'.\n",
    "When this parameter is modified, your matrix needs to be rebuilt.\n",
    "\"\"\"\n",
    "\n",
    "# data_type = 'test'\n",
    "data_type = 'train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c936e3f-fb0f-4d52-be8b-197cfaec6157",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_plus(counter, number):\n",
    "    ans = Counter()\n",
    "    while number!=0:\n",
    "        if number%2==1:\n",
    "            ans += counter\n",
    "        number = number // 2\n",
    "        if number!=0:\n",
    "            counter = counter+counter\n",
    "    return ans\n",
    "\n",
    "train_data = pd.read_pickle('../data/train_data_005.dataset').reset_index(drop=True)\n",
    "valid_data = pd.read_pickle('../data/valid_data_005.dataset').reset_index(drop=True)\n",
    "test_data = pd.read_pickle('../data/test_data_p2.dataset').reset_index(drop=True)\n",
    "test_data2 = pd.read_pickle('../data/test_data.dataset').reset_index(drop=True)\n",
    "test_data3 = pd.read_pickle('../data/test_data_p3.dataset').reset_index(drop=True)\n",
    "test_data2 = pd.concat([test_data2,test_data3]).reset_index(drop=True)\n",
    "train_data['user'] = range(len(train_data))\n",
    "train_data['user'] = train_data['user'].astype('int32')\n",
    "valid_data['user'] = range(len(train_data),len(train_data)+len(valid_data))\n",
    "valid_data['user'] = valid_data['user'].astype('int32')\n",
    "test_data['user'] = range(len(train_data)+len(valid_data),len(train_data)+len(valid_data)+len(test_data))\n",
    "test_data['user'] = test_data['user'].astype('int32')\n",
    "train_session = train_data['session'].tolist()\n",
    "train_locale = train_data['locale'].tolist()\n",
    "train_user = train_data['user'].tolist()\n",
    "valid_session = valid_data['session'].tolist()\n",
    "valid_locale = valid_data['locale'].tolist()\n",
    "valid_user = valid_data['user'].tolist()\n",
    "test_session = test_data['session'].tolist()\n",
    "test_locale = test_data['locale'].tolist()\n",
    "test_user = test_data['user'].tolist()\n",
    "test_session2 = test_data2['session'].tolist()\n",
    "test_locale2 = test_data2['locale'].tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e1f9de58-d353-4be6-b8ae-e2d045c49450",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### string rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a070ec92-9700-441f-bf17-77b9cc2ad683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [01:36<00:00, 33874.75it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 63418.41it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 432559.82it/s]\n",
      "100%|██████████| 3256352/3256352 [00:47<00:00, 68041.71it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 70525.97it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 319655.49it/s]\n"
     ]
    }
   ],
   "source": [
    "check1 = {}\n",
    "counter1 = Counter()\n",
    "train_string1 = []\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    string = \"\"\n",
    "    row = train_session[i]\n",
    "    pre = row[:-1]\n",
    "    for item in pre[::-1]:\n",
    "        string = string + str(item)+' '\n",
    "        if counter1[string] == 0:\n",
    "            counter1[string] +=1\n",
    "            check1[string] = Counter()\n",
    "        else:\n",
    "            counter1[string] +=1\n",
    "        check1[string][row[-1]] += 1\n",
    "    train_string1.append(string)\n",
    "train_data['string1'] = train_string1\n",
    "\n",
    "valid_string1 = []\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    string = \"\"\n",
    "    row = valid_session[i]\n",
    "    pre = row[:-1]\n",
    "    for item in pre[::-1]:\n",
    "        string = string + str(item)+' '\n",
    "        if counter1[string] == 0:\n",
    "            counter1[string] +=1\n",
    "            check1[string] = Counter()\n",
    "        else:\n",
    "            counter1[string] +=1\n",
    "        check1[string][row[-1]] += 1\n",
    "    valid_string1.append(string)\n",
    "valid_data['string1'] = valid_string1\n",
    "    \n",
    "test_string1 = []\n",
    "for i in tqdm(range(len(test_session))):\n",
    "    string = \"\"\n",
    "    row = test_session[i]\n",
    "    pre = row\n",
    "    for item in pre[::-1]:\n",
    "        string = string + str(item)+' '\n",
    "    test_string1.append(string)\n",
    "test_data['string1'] = test_string1\n",
    "\n",
    "check3 = {}\n",
    "counter3 = Counter()\n",
    "train_string3 = []\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    string = \"\"\n",
    "    row = train_session[i]\n",
    "    pre = row[:-1]\n",
    "    pre = list(dict.fromkeys(pre[::-1]))[::-1]\n",
    "    for item in pre[::-1]:\n",
    "        string = string + str(item)+' '\n",
    "        if counter3[string] == 0:\n",
    "            counter3[string] +=1\n",
    "            check3[string] = Counter()\n",
    "        else:\n",
    "            counter3[string] +=1\n",
    "        check3[string][row[-1]] += 1\n",
    "    train_string3.append(string)\n",
    "train_data['string3'] = train_string3\n",
    "valid_string3 = []\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    string = \"\"\n",
    "    row = valid_session[i]\n",
    "    pre = row[:-1]\n",
    "    pre = list(dict.fromkeys(pre[::-1]))[::-1]\n",
    "    for item in pre[::-1]:\n",
    "        string = string + str(item)+' '\n",
    "        if counter3[string] == 0:\n",
    "            counter3[string] +=1\n",
    "            check3[string] = Counter()\n",
    "        else:\n",
    "            counter3[string] +=1\n",
    "        check3[string][row[-1]] += 1\n",
    "    valid_string3.append(string)\n",
    "valid_data['string3'] = valid_string3\n",
    "\n",
    "test_string3 = []\n",
    "for i in tqdm(range(len(test_session))):\n",
    "    string = \"\"\n",
    "    row = test_session[i]\n",
    "    pre = row\n",
    "    pre = list(dict.fromkeys(pre[::-1]))[::-1]\n",
    "    for item in pre[::-1]:\n",
    "        string = string + str(item)+' '\n",
    "    test_string3.append(string)\n",
    "test_data['string3'] = test_string3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9fdd0f1-b96d-4463-866a-a30e7fa54872",
   "metadata": {
    "tags": []
   },
   "source": [
    "### process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c585bd-8ef1-48ca-aac0-ee9fdb5e5e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:10<00:00, 319489.32it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 362491.50it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 438015.07it/s]\n",
      "100%|██████████| 376971/376971 [00:01<00:00, 264053.02it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(len(train_session))):\n",
    "    row = []\n",
    "    for j in range(len(train_session[i])):\n",
    "        if j != 0:\n",
    "            if train_session[i][j] == train_session[i][j-1]:\n",
    "                continue\n",
    "        row.append(train_session[i][j])\n",
    "    train_session[i] = row\n",
    "train_data['session'] = train_session\n",
    "    \n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = []\n",
    "    for j in range(len(valid_session[i])):\n",
    "        if j != 0:\n",
    "            if valid_session[i][j] == valid_session[i][j-1]:\n",
    "                continue\n",
    "        row.append(valid_session[i][j])\n",
    "    valid_session[i] = row\n",
    "valid_data['session'] = valid_session\n",
    "\n",
    "for i in tqdm(range(len(test_session))):\n",
    "    row = []\n",
    "    for j in range(len(test_session[i])):\n",
    "        if j != 0:\n",
    "            if test_session[i][j] == test_session[i][j-1]:\n",
    "                continue\n",
    "        row.append(test_session[i][j])\n",
    "    test_session[i] = row\n",
    "test_data['session'] = test_session\n",
    "\n",
    "for i in tqdm(range(len(test_session2))):\n",
    "    row = []\n",
    "    for j in range(len(test_session2[i])):\n",
    "        if j != 0:\n",
    "            if test_session2[i][j] == test_session2[i][j-1]:\n",
    "                continue\n",
    "        row.append(test_session2[i][j])\n",
    "    test_session2[i] = row\n",
    "test_data2['session'] = test_session2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d35a86e-c3bf-49da-851a-e76c7525009c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# init feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34bab1d-ff3f-44d1-bb10-8a53940a1ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b20f408-e08f-4e17-9ebd-427195d1cfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemnum = 1410675 + 1\n",
    "#locale map number\n",
    "locale_map = {}\n",
    "temp = 0\n",
    "for item in list(set(train_locale)):\n",
    "    locale_map[item] = temp\n",
    "    temp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3631cf3-5482-41c1-91b1-e88d79a1e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max(cov,number):\n",
    "    max_ = [[ 0 for _ in range(number)] for i in range(3)]\n",
    "    for i in range(3):\n",
    "        for j in range(0,number):\n",
    "            max_[i][j] = sum(cov[i][j].values())\n",
    "    return max_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "557293f9-2987-4af6-8bee-cd9bd2b2307d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### price feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314b11e-57e0-4eb5-ace6-38eab76f0867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE 9180.0\n",
      "JP 3933800.0\n",
      "UK 7999.0\n"
     ]
    }
   ],
   "source": [
    "price = pro_df[pro_df['locale']=='DE']['price'].tolist()\n",
    "for i in range(len(price)):\n",
    "    if price[i] == 40000000.07:\n",
    "        price[i] = -1\n",
    "print('DE',max(price))\n",
    "price = pro_df[pro_df['locale']=='JP']['price'].tolist()\n",
    "for i in range(len(price)):\n",
    "    if price[i] == 40000000.07:\n",
    "        price[i] = -1\n",
    "print('JP',max(price))\n",
    "price = pro_df[pro_df['locale']=='UK']['price'].tolist()\n",
    "for i in range(len(price)):\n",
    "    if price[i] == 40000000.07:\n",
    "        price[i] = -1\n",
    "print('UK',max(price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431ff660-c404-4923-ad86-78cfe26fc504",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518327/518327 [00:00<00:00, 1442108.91it/s]\n",
      "100%|██████████| 395009/395009 [00:00<00:00, 1446204.54it/s]\n",
      "100%|██████████| 500180/500180 [00:00<00:00, 1436405.09it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for locale_ in ['DE','JP','UK']:\n",
    "    price = []\n",
    "    item = []\n",
    "    temp = 1.\n",
    "    max_price = 9180\n",
    "    min_price = 0.01\n",
    "    if locale_ == 'JP':\n",
    "        max_price = 3933800\n",
    "        min_price = 1\n",
    "    elif locale_=='UK':\n",
    "        min_price = 0.01\n",
    "        max_price = 7999\n",
    "\n",
    "    temp_df = pro_df[pro_df['locale']==locale_]\n",
    "    temp_price = temp_df['price'].tolist()\n",
    "    temp_item = temp_df['item'].tolist()\n",
    "    for i in tqdm(range(len(temp_price))):\n",
    "        item.append(temp_item[i])\n",
    "        if temp_price[i] == 40000000.07:\n",
    "            price.append(1)\n",
    "        elif temp_price[i] == 0:\n",
    "            price.append(min_price/max_price)\n",
    "        else:\n",
    "            price.append(temp_price[i]/max_price)\n",
    "    price_df = pd.DataFrame({'item':item,'price':price}).set_index([\"item\"])\n",
    "    price_df['price'] = price_df['price'].astype('float32')\n",
    "    price_df.to_parquet('item_price'+locale_+'.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967606ab-ecea-44e9-b32d-33fc13aa1b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:09<00:00, 338661.94it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 321640.88it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 368050.64it/s]\n",
      "100%|██████████| 3256352/3256352 [00:08<00:00, 392459.79it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 409085.76it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 418125.39it/s] \n",
      "100%|██████████| 3256352/3256352 [00:09<00:00, 337819.68it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 336122.74it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 349824.88it/s] \n"
     ]
    }
   ],
   "source": [
    "for locale_ in ['DE','JP','UK']:\n",
    "    new_user = []\n",
    "    price_power_mean = []\n",
    "    price_power_1_mean = []\n",
    "    price_power_2_mean = []\n",
    "    price_power_3_mean = []\n",
    "    price_power_5_mean = []\n",
    "    \n",
    "    price_df = pd.read_parquet('item_price'+locale_+'.pqt')\n",
    "    price_dict = dict(zip(price_df.index.tolist(), price_df['price'].tolist()))\n",
    "\n",
    "    for i in tqdm(range(len(train_session))):\n",
    "        if train_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(train_user[i])\n",
    "        row = train_session[i][:-1]\n",
    "        row = list(dict.fromkeys(row[::-1]))\n",
    "        scores = 0.\n",
    "        scores1 = 0.\n",
    "        scores2 = 0.\n",
    "        scores3 = 0.\n",
    "        scores5 = 0.\n",
    "        for j in row:\n",
    "            scores +=  price_dict[j]\n",
    "        for j in row[:1]:\n",
    "            scores1 +=  price_dict[j]\n",
    "        for j in row[:2]:\n",
    "            scores2 +=  price_dict[j]\n",
    "        for j in row[:3]:\n",
    "            scores3 +=  price_dict[j]\n",
    "        for j in row[:5]:\n",
    "            scores5 +=  price_dict[j]\n",
    "        scores/=len(row)\n",
    "        scores1/=len(row[:1])\n",
    "        scores2/=len(row[:2])\n",
    "        scores3/=len(row[:3])\n",
    "        scores5/=len(row[:5])\n",
    "        price_power_mean.append(scores)\n",
    "        price_power_1_mean.append(scores1)\n",
    "        price_power_2_mean.append(scores2)\n",
    "        price_power_3_mean.append(scores3)\n",
    "        price_power_5_mean.append(scores5)\n",
    "        \n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        if valid_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(valid_user[i])\n",
    "        row = valid_session[i][:-1]\n",
    "        row = list(dict.fromkeys(row[::-1]))\n",
    "        scores = 0.\n",
    "        scores = 0.\n",
    "        scores1 = 0.\n",
    "        scores2 = 0.\n",
    "        scores3 = 0.\n",
    "        scores5 = 0.\n",
    "        for j in row:\n",
    "            scores +=  price_dict[j]\n",
    "        for j in row[:1]:\n",
    "            scores1 +=  price_dict[j]\n",
    "        for j in row[:2]:\n",
    "            scores2 +=  price_dict[j]\n",
    "        for j in row[:3]:\n",
    "            scores3 +=  price_dict[j]\n",
    "        for j in row[:5]:\n",
    "            scores5 +=  price_dict[j]\n",
    "        scores/=len(row)\n",
    "        scores1/=len(row[:1])\n",
    "        scores2/=len(row[:2])\n",
    "        scores3/=len(row[:3])\n",
    "        scores5/=len(row[:5])\n",
    "        price_power_mean.append(scores)\n",
    "        price_power_1_mean.append(scores1)\n",
    "        price_power_2_mean.append(scores2)\n",
    "        price_power_3_mean.append(scores3)\n",
    "        price_power_5_mean.append(scores5)\n",
    "        \n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        if test_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(test_user[i])\n",
    "        row = test_session[i]\n",
    "        row = list(dict.fromkeys(row[::-1]))\n",
    "        scores = 0.\n",
    "        scores1 = 0.\n",
    "        scores2 = 0.\n",
    "        scores3 = 0.\n",
    "        scores5 = 0.\n",
    "        for j in row:\n",
    "            scores +=  price_dict[j]\n",
    "        for j in row[:1]:\n",
    "            scores1 +=  price_dict[j]\n",
    "        for j in row[:2]:\n",
    "            scores2 +=  price_dict[j]\n",
    "        for j in row[:3]:\n",
    "            scores3 +=  price_dict[j]\n",
    "        for j in row[:5]:\n",
    "            scores5 +=  price_dict[j]\n",
    "        scores/=len(row)\n",
    "        scores1/=len(row[:1])\n",
    "        scores2/=len(row[:2])\n",
    "        scores3/=len(row[:3])\n",
    "        scores5/=len(row[:5])\n",
    "        price_power_mean.append(scores)\n",
    "        price_power_1_mean.append(scores1)\n",
    "        price_power_2_mean.append(scores2)\n",
    "        price_power_3_mean.append(scores3)\n",
    "        price_power_5_mean.append(scores5)\n",
    "        \n",
    "    user_price = pd.DataFrame({'user':new_user,\n",
    "                               'price_power_0_mean':price_power_mean,\n",
    "                               'price_power_1_mean':price_power_1_mean,\n",
    "                               'price_power_2_mean':price_power_2_mean,\n",
    "                               'price_power_3_mean':price_power_3_mean,\n",
    "                               'price_power_5_mean':price_power_5_mean,\n",
    "                              }).set_index([\"user\"])\n",
    "    user_price['price_power_0_mean'] = user_price['price_power_0_mean'].astype('float32')\n",
    "    user_price['price_power_1_mean'] = user_price['price_power_1_mean'].astype('float32')\n",
    "    user_price['price_power_2_mean'] = user_price['price_power_2_mean'].astype('float32')\n",
    "    user_price['price_power_3_mean'] = user_price['price_power_3_mean'].astype('float32')\n",
    "    user_price['price_power_5_mean'] = user_price['price_power_5_mean'].astype('float32')\n",
    "    user_price.to_parquet('user_price'+locale_+'.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "271dac78",
   "metadata": {},
   "source": [
    "## matrix feature"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e7e9e2c-4227-4d97-ba74-278fcc351459",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.next item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93657b35-2ed9-48d6-86f1-9feeb22f8060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next(cov,row,locale__,apper,_max=None):\n",
    "    for j in range(len(row)-1):\n",
    "        cov[locale__][row[j]][row[j+1]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d2313-47dd-4e96-a981-3ed5c34eefed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:14<00:00, 228792.85it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 214977.31it/s]\n",
      "100%|██████████| 316972/316972 [00:01<00:00, 298007.28it/s]\n",
      "100%|██████████| 376971/376971 [00:01<00:00, 306261.07it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next = [[ Counter() for _ in range(itemnum)] for i in range(3)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next(item_next,row,locale__,1)\n",
    "        \n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_next(item_next,row,locale__,1)\n",
    "    \n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_item_next(item_next,row,locale__,3)\n",
    "\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_item_next(item_next,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efca0a5-967a-47f5-95a7-155515787b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next_max = get_max(item_next,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3858c5c7-a14c-4858-8cb0-de70505253a8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2.next item-skip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3d5d2-d1fa-4190-aa27-2344d1674db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next2(cov,row,locale__,apper,_max=None): \n",
    "    for j in range(len(row)-2):\n",
    "        cov[locale__][row[j]][row[j+2]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6251ac-8f59-43ef-b6e3-33b179ea846f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:13<00:00, 243982.76it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 225399.06it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 324879.46it/s]\n",
      "100%|██████████| 376971/376971 [00:01<00:00, 337179.90it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next2 = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next2(item_next2,row,locale__,1)\n",
    "        \n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_next2(item_next2,row,locale__,1)\n",
    "    \n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_item_next2(item_next2,row,locale__,2)\n",
    "        \n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_item_next2(item_next2,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1184619c-c9ce-4838-9ed0-fa81e9ea4041",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next2_max = get_max(item_next2,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02697d03-0965-479b-9b96-bda94e2b1354",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 3.next item-skip3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c509d-b41f-445c-a0bb-7ea8ec3177d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next3(cov,row,locale__,apper,_max=None): \n",
    "    for j in range(len(row)-3):\n",
    "        cov[locale__][row[j]][row[j+3]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f857fd-a6c5-4e2e-8225-23cc811b4ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:10<00:00, 320211.41it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 304914.26it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 434469.44it/s]\n",
      "100%|██████████| 376971/376971 [00:00<00:00, 447324.85it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next3 = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next3(item_next3,row,locale__,1)\n",
    "        \n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_next3(item_next3,row,locale__,1)\n",
    "    \n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_item_next3(item_next3,row,locale__,2)\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_item_next3(item_next3,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e32217-2540-45f3-bbb3-ef5c658c2c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next3_max = get_max(item_next3,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa6cf2c8-fa45-481f-a5a9-53153bfe9c83",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4.back item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759476e1-f6a6-4bf3-ad2a-b7758dfaab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next_back(cov,row,locale__,apper,_max=None):\n",
    "    for j in range(len(row)-1):\n",
    "        cov[locale__][row[j+1]][row[j]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j+1]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442f82c-1b4c-4686-8465-dfc5ef7b3d6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:15<00:00, 203934.12it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 192166.66it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next_back = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next_back(item_next_back,row,locale__,1)\n",
    "    \n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_next_back(item_next_back,row,locale__,1)\n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_item_next_back(item_next_back,row,locale__,2)\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_item_next_back(item_next_back,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f3395f-7d07-45cd-ab5d-fe350eab8b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next_back_max = get_max(item_next_back,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "446a804a-6fb6-483e-a343-1aa8772dfacf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 5.back item-skip2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135439b8-b7b5-4cb9-a049-5f2c13d78041",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next2_back(cov,row,locale__,apper,_max=None): \n",
    "    for j in range(len(row)-2):\n",
    "        cov[locale__][row[j+2]][row[j]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j+2]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb860e87-2e57-48ca-ace4-1a91462e0405",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:13<00:00, 234267.65it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 208605.50it/s]\n",
      "100%|██████████| 316972/316972 [00:01<00:00, 304463.48it/s]\n",
      "100%|██████████| 376971/376971 [00:01<00:00, 316030.89it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next2_back = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next2_back(item_next2_back,row,locale__,1)\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_next2_back(item_next2_back,row,locale__,1)\n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_item_next2_back(item_next2_back,row,locale__,2)\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_item_next2_back(item_next2_back,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d43d14e-05b7-4f83-a69e-eb6e6d910072",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next2_back_max = get_max(item_next2_back,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d55c8631-b2c6-4534-938d-6e7ab5738edc",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 6.last item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92152500-9c13-4ad7-9dd7-b7ee9de09944",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next_last(cov,row,locale__,apper,_max=None): \n",
    "    for j in range(len(row)-1):\n",
    "        cov[locale__][row[j]][row[-1]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1397e9-9647-44c1-a20c-7a32b0f0d0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:22<00:00, 146006.02it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 143843.69it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next_last_0_2 = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "item_next_last_2_3 = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "# item_next_last_3_5 = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "item_next_last_5_5 = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next_last(item_next_last_0_2,row[-2:],locale__,1)\n",
    "    make_item_next_last(item_next_last_2_3,row[-3:-2] + [row[-1]],locale__,1)\n",
    "    # make_item_next_last(item_next_last_3_5,row[-5:-3]+ [row[-1]],locale__,1)\n",
    "    make_item_next_last(item_next_last_5_5,row[:-3]+ [row[-1]],locale__,1)\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_next_last(item_next_last_0_2,row[-2:],locale__,1)\n",
    "    make_item_next_last(item_next_last_2_3,row[-3:-2] + [row[-1]],locale__,1)\n",
    "    # make_item_next_last(item_next_last_3_5,row[-5:-3]+ [row[-1]],locale__,1)\n",
    "    make_item_next_last(item_next_last_5_5,row[:-3]+ [row[-1]],locale__,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ea7e47-dace-4760-a707-f6c107da33c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next_last_0_2_max = get_max(item_next_last_0_2,itemnum)\n",
    "item_next_last_2_3_max = get_max(item_next_last_2_3,itemnum)\n",
    "# item_next_last_3_5_max = get_max(item_next_last_3_5,itemnum)\n",
    "item_next_last_5_5_max = get_max(item_next_last_5_5,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea11ef22-7d1e-49a2-97b8-ac0d5ff5e420",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 7.time last item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa85aedd-f1dd-4891-b6f3-d72009c3d516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:25<00:00, 126318.34it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 117590.93it/s]\n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "\n",
    "item_last_time = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "def make_item_last_time(cov,row,locale__,apper,_max=None):\n",
    "    weight = 0.\n",
    "    temp_list = [1,2,3,5,20]\n",
    "    for j in range(2,len(row)+1):\n",
    "        if j >10:\n",
    "            break\n",
    "        for k in range(len(temp_list)):\n",
    "            if j-1<= temp_list[k]:\n",
    "                weight = 1.0/pow(20,k)\n",
    "                break\n",
    "        cov[locale__][row[-j]][row[-1]] += apper*weight\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[-j]] += apper*weight\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_last_time(item_last_time,row,locale__,1)\n",
    "            \n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_last_time(item_last_time,row,locale__,1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce4c656-a48d-4541-8c6d-e829e4c8d8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_last_time_max = get_max(item_last_time,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2eb527eb-3266-439f-8807-aafc54d03c6f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 8.cf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c53a82b-cb55-4bc7-ab5b-7f4a93b73d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [01:41<00:00, 32052.70it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 40270.41it/s]\n",
      "100%|██████████| 1410676/1410676 [00:08<00:00, 162821.38it/s]\n",
      "100%|██████████| 1410676/1410676 [00:09<00:00, 152622.33it/s]\n",
      "100%|██████████| 1410676/1410676 [00:09<00:00, 146360.18it/s] \n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "item_sim = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "item_cnt = [[ 0 for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    if i%10==0 and data_type=='train':\n",
    "        row = train_session[i][:-1]\n",
    "    else:\n",
    "        row = train_session[i]\n",
    "    locale_ = locale_map[train_locale[i]]\n",
    "    for j in range(len(row)):\n",
    "        item_cnt[locale_][row[j]]+=1\n",
    "        # right = list(dict.fromkeys(row[j:]))\n",
    "        # left = list(dict.fromkeys(row[:j+1:-1]))\n",
    "        right = row[j:]\n",
    "        left = row[:j+1:-1]\n",
    "        le = len(right)+len(left) - 2\n",
    "        if le == 0:\n",
    "            continue\n",
    "        for k in range(1,len(right)):\n",
    "            loc_weight = 0.8**(k-1)\n",
    "            item_sim[locale_][row[j]][right[k]] += loc_weight/math.log(1 + le)\n",
    "        for k in range(1,len(left)):\n",
    "            loc_weight = 0.7**(k-1)\n",
    "            item_sim[locale_][row[j]][left[k]] += loc_weight/math.log(1 + le)\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i][:-1]\n",
    "    locale_ = locale_map[valid_locale[i]]\n",
    "    for j in range(len(row)):\n",
    "        item_cnt[locale_][row[j]]+=1\n",
    "        # right = list(dict.fromkeys(row[j:]))\n",
    "        # left = list(dict.fromkeys(row[:j+1:-1]))\n",
    "        right = row[j:]\n",
    "        left = row[:j+1:-1]\n",
    "        le = len(right)+len(left) - 2\n",
    "        if le == 0:\n",
    "            continue\n",
    "        for k in range(1,len(right)):\n",
    "            loc_weight = 0.8**(k-1)\n",
    "            item_sim[locale_][row[j]][right[k]] += loc_weight/math.log(1 + le)\n",
    "        for k in range(1,len(left)):\n",
    "            loc_weight = 0.7**(k-1)\n",
    "            item_sim[locale_][row[j]][left[k]] += loc_weight/math.log(1 + le)\n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale_ = locale_map[test_locale[i]]\n",
    "        for j in range(len(row)):\n",
    "            item_cnt[locale_][row[j]]+=1\n",
    "            # right = list(dict.fromkeys(row[j:]))\n",
    "            # left = list(dict.fromkeys(row[:j+1:-1]))\n",
    "            right = row[j:]\n",
    "            left = row[:j+1:-1]\n",
    "            le = len(right)+len(left) - 2\n",
    "            if le == 0:\n",
    "                continue\n",
    "            for k in range(1,len(right)):\n",
    "                loc_weight = 0.8**(k-1)\n",
    "                item_sim[locale_][row[j]][right[k]] += loc_weight/math.log(1 + le)\n",
    "            for k in range(1,len(left)):\n",
    "                loc_weight = 0.7**(k-1)\n",
    "                item_sim[locale_][row[j]][left[k]] += loc_weight/math.log(1 + le)\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale_ = locale_map[test_locale2[i]]\n",
    "        for j in range(len(row)):\n",
    "            item_cnt[locale_][row[j]]+=1\n",
    "            # right = list(dict.fromkeys(row[j:]))\n",
    "            # left = list(dict.fromkeys(row[:j+1:-1]))\n",
    "            right = row[j:]\n",
    "            left = row[:j+1:-1]\n",
    "            le = len(right)+len(left) - 2\n",
    "            if le == 0:\n",
    "                continue\n",
    "            for k in range(1,len(right)):\n",
    "                loc_weight = 0.8**(k-1)\n",
    "                item_sim[locale_][row[j]][right[k]] += loc_weight/math.log(1 + le)\n",
    "            for k in range(1,len(left)):\n",
    "                loc_weight = 0.7**(k-1)\n",
    "                item_sim[locale_][row[j]][left[k]] += loc_weight/math.log(1 + le)\n",
    "for locale_ in range(temp):\n",
    "    for i in tqdm(range(itemnum)):\n",
    "        for j in item_sim[locale_][i].keys():\n",
    "            item_sim[locale_][i][j] = item_sim[locale_][i][j]/ math.sqrt(item_cnt[locale_][i] * item_cnt[locale_][j])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b796fbc-ce07-4a08-9e7d-5ce89d7f79dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 9.unique_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae9aa53-f7fe-49b5-9847-e12d659d26a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [01:00<00:00, 53753.98it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 48714.62it/s]\n",
      "100%|██████████| 316972/316972 [00:04<00:00, 74161.99it/s]\n",
      "100%|██████████| 376971/376971 [00:04<00:00, 77471.66it/s] \n"
     ]
    }
   ],
   "source": [
    "#train data\n",
    "unique_pair = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "def make_unique_pair(cov,row,locale__,apper,_max=None):\n",
    "    row = list(set(row))\n",
    "    for j in range(len(row)):\n",
    "        for k in range(j+1,len(row)):\n",
    "            cov[locale__][row[j]][row[k]] += apper\n",
    "            cov[locale__][row[k]][row[j]] += apper\n",
    "            if _max !=None:\n",
    "                _max[locale__][row[j]]+=apper\n",
    "                _max[locale__][row[k]]+=apper\n",
    "apper = 1\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_unique_pair(unique_pair,row,locale__,1)\n",
    "\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_unique_pair(unique_pair,row,locale__,1)\n",
    "if data_type == 'test':\n",
    "    apper = 2\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_unique_pair(unique_pair,row,locale__,2)\n",
    "        \n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_unique_pair(unique_pair,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af13c87-d144-4c64-99bc-f4bf89fe2785",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_pair_max = get_max(unique_pair,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4476b597-ad1e-4b17-829a-446cfb441401",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 10.brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305e66b0-405e-448b-ba79-e7f70437e4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:33<00:00, 97412.26it/s] \n",
      "100%|██████████| 16364/16364 [00:00<00:00, 97756.60it/s]\n",
      "100%|██████████| 316972/316972 [00:02<00:00, 146379.96it/s]\n",
      "100%|██████████| 376971/376971 [00:02<00:00, 149676.28it/s]\n"
     ]
    }
   ],
   "source": [
    "apper = 1\n",
    "brand = pro_df['brand'].tolist()\n",
    "brand = [str(x) for x in brand]\n",
    "brand_key = list(set(brand))\n",
    "brand_number = len(brand_key)\n",
    "brand_values = list(range(1,len(brand_key)+1))\n",
    "brand_s2n = dict(zip(brand_key, brand_values))\n",
    "brand_s2n['nan'] = 0\n",
    "brand_map = [{} for i in range(temp)]\n",
    "brand_locale = pro_df['locale'].tolist()\n",
    "brand_item = pro_df['item'].tolist()\n",
    "\n",
    "for i in range(len(brand)):\n",
    "    if brand_locale[i] not in ['DE','UK','JP']:\n",
    "        continue\n",
    "    brand_map[locale_map[brand_locale[i]]][brand_item[i]]= brand_s2n[brand[i]]\n",
    "\n",
    "brand_window = [[ Counter() for _ in range(brand_number+2)] for i in range(temp)]\n",
    "def make_brand_window(cov,row,locale__,apper,max_=None):\n",
    "    for j in range(len(row)-1):\n",
    "        a = brand_map[locale__][row[j]]\n",
    "        b = brand_map[locale__][row[j+1]]\n",
    "        if a*b==0:\n",
    "            continue\n",
    "        cov[locale__][a][b] += apper\n",
    "        cov[locale__][b][a] += apper\n",
    "        if max_!=None:\n",
    "            max_[locale__][a] += apper\n",
    "            max_[locale__][b] += apper\n",
    "    for j in range(len(row)-2):\n",
    "        a = brand_map[locale__][row[j]]\n",
    "        b = brand_map[locale__][row[j+2]]\n",
    "        if a*b==0:\n",
    "            continue\n",
    "        cov[locale__][a][b] += apper\n",
    "        cov[locale__][b][a] += apper\n",
    "        if max_!=None:\n",
    "            max_[locale__][a] += apper\n",
    "            max_[locale__][b] += apper\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_brand_window(brand_window,row,locale__,1)\n",
    "\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_brand_window(brand_window,row,locale__,1)\n",
    "\n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_brand_window(brand_window,row,locale__,2)\n",
    "\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_brand_window(brand_window,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9fb67b-92f6-4798-93ae-b10ab2e8cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:05<00:00, 603823.10it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 584192.35it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 711295.99it/s]\n",
      "100%|██████████| 3256352/3256352 [00:04<00:00, 698089.15it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 682900.43it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 773557.51it/s] \n",
      "100%|██████████| 3256352/3256352 [00:05<00:00, 574148.44it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 563839.27it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 652790.44it/s] \n"
     ]
    }
   ],
   "source": [
    "for locale_ in ['DE','JP','UK']:\n",
    "    new_user = []\n",
    "    brand_ratio = []\n",
    "    brand_num = []\n",
    "    brand_item = []\n",
    "    locale__ = locale_map[locale_]\n",
    "    for i in tqdm(range(len(train_session))):\n",
    "        \n",
    "        if train_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(train_user[i])\n",
    "        row = train_session[i][:-1]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if brand_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        brand_item.append(len(c))\n",
    "        brand_num.append(len(set(c)))\n",
    "        brand_ratio.append(len(c)/len(row))\n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        if valid_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(valid_user[i])\n",
    "        row = valid_session[i][:-1]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if brand_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        brand_item.append(len(c))\n",
    "        brand_num.append(len(set(c)))\n",
    "        brand_ratio.append(len(c)/len(row))\n",
    "        \n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        if test_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(test_user[i])\n",
    "        row = test_session[i]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if brand_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        brand_item.append(len(c))\n",
    "        brand_num.append(len(set(c)))\n",
    "        brand_ratio.append(len(c)/len(row))\n",
    "    user_brand = pd.DataFrame({'user':new_user,\n",
    "                             'brand_item':brand_item,\n",
    "                             'brand_num':brand_num,\n",
    "                             'brand_ratio':brand_ratio,\n",
    "                            }).set_index([\"user\"])\n",
    "    user_brand.to_parquet('user_brand'+locale_+'.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3086d7f-2268-4339-a3be-92d94af0123f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 11.author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7b83be-cdaf-4eee-a653-a086a6ed4b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:14<00:00, 230784.30it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 216524.94it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 321164.83it/s]\n",
      "100%|██████████| 376971/376971 [00:01<00:00, 323463.89it/s]\n"
     ]
    }
   ],
   "source": [
    "apper = 1\n",
    "author = pro_df['author'].tolist()\n",
    "author = [str(x) for x in author]\n",
    "author_key = list(set(author))\n",
    "author_number = len(author_key)\n",
    "author_values = list(range(1,len(author_key)+1))\n",
    "author_s2n = dict(zip(author_key, author_values))\n",
    "author_s2n['nan'] = 0\n",
    "author_map = [{} for i in range(temp)]\n",
    "author_locale = pro_df['locale'].tolist()\n",
    "author_item = pro_df['item'].tolist()\n",
    "\n",
    "for i in range(len(author)):\n",
    "    if author_locale[i] not in ['DE','UK','JP']:\n",
    "        continue\n",
    "    author_map[locale_map[author_locale[i]]][author_item[i]]= author_s2n[author[i]]\n",
    "\n",
    "author_window = [[ Counter() for _ in range(author_number+2)] for i in range(temp)]\n",
    "def make_author_window(cov,row,locale__,apper,max_=None):\n",
    "    for j in range(len(row)-1):\n",
    "        a = author_map[locale__][row[j]]\n",
    "        b = author_map[locale__][row[j+1]]\n",
    "        if a*b==0:\n",
    "            continue\n",
    "        cov[locale__][a][b] += apper\n",
    "        cov[locale__][b][a] += apper\n",
    "        if max_!=None:\n",
    "            max_[locale__][a] += apper\n",
    "            max_[locale__][b] += apper\n",
    "    for j in range(len(row)-2):\n",
    "        a = author_map[locale__][row[j]]\n",
    "        b = author_map[locale__][row[j+2]]\n",
    "        if a*b==0:\n",
    "            continue\n",
    "        cov[locale__][a][b] += apper\n",
    "        cov[locale__][b][a] += apper\n",
    "        if max_!=None:\n",
    "            max_[locale__][a] += apper\n",
    "            max_[locale__][b] += apper\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_author_window(author_window,row,locale__,1)\n",
    "\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_author_window(author_window,row,locale__,1)\n",
    "if data_type=='test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_author_window(author_window,row,locale__,2)\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_author_window(author_window,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b540a5fe-2a90-4aa2-84dd-5c52e73ec8f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:04<00:00, 714811.25it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 660548.29it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 808631.14it/s]\n",
      "100%|██████████| 3256352/3256352 [00:04<00:00, 802030.92it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 765449.84it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 870836.80it/s] \n",
      "100%|██████████| 3256352/3256352 [00:04<00:00, 713022.48it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 675175.01it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 777808.48it/s] \n"
     ]
    }
   ],
   "source": [
    "for locale_ in ['DE','JP','UK']:\n",
    "    new_user = []\n",
    "    author_ratio = []\n",
    "    author_num = []\n",
    "    author_item = []\n",
    "    locale__ = locale_map[locale_]\n",
    "    for i in tqdm(range(len(train_session))):\n",
    "        \n",
    "        if train_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(train_user[i])\n",
    "        row = train_session[i][:-1]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if author_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        author_item.append(len(c))\n",
    "        author_num.append(len(set(c)))\n",
    "        author_ratio.append(len(c)/len(row))\n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        if valid_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(valid_user[i])\n",
    "        row = valid_session[i][:-1]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if author_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        author_item.append(len(c))\n",
    "        author_num.append(len(set(c)))\n",
    "        author_ratio.append(len(c)/len(row))\n",
    "        \n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        if test_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(test_user[i])\n",
    "        row = test_session[i]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if author_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        author_item.append(len(c))\n",
    "        author_num.append(len(set(c)))\n",
    "        author_ratio.append(len(c)/len(row))\n",
    "    user_pop = pd.DataFrame({'user':new_user,\n",
    "                             'author_item':author_item,\n",
    "                             'author_num':author_num,\n",
    "                             'author_ratio':author_ratio,\n",
    "                            }).set_index([\"user\"])\n",
    "    user_pop.to_parquet('user_author'+locale_+'.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7748482-8f00-4f64-8431-dbfc9fb8a237",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 12.material"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3266a2-2eb9-4a42-8c59-3a1cca2c933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_material_window(cov,row,locale__,apper,max_=None):\n",
    "    for j in range(len(row)-1):\n",
    "        a = material_map[locale__][row[j]]\n",
    "        b = material_map[locale__][row[j+1]]\n",
    "        if a*b==0:\n",
    "            continue\n",
    "        cov[locale__][a][b] += apper\n",
    "        cov[locale__][b][a] += apper\n",
    "        if max_!=None:\n",
    "            max_[locale__][a] += apper\n",
    "            max_[locale__][b] += apper\n",
    "    for j in range(len(row)-2):\n",
    "        a = material_map[locale__][row[j]]\n",
    "        b = material_map[locale__][row[j+2]]\n",
    "        if a*b==0:\n",
    "            continue\n",
    "        cov[locale__][a][b] += apper\n",
    "        cov[locale__][b][a] += apper\n",
    "        if max_!=None:\n",
    "            max_[locale__][a] += apper\n",
    "            max_[locale__][b] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6245dad-fb17-42b6-9abb-cbcad65fea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:20<00:00, 157869.14it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 150836.84it/s]\n",
      "100%|██████████| 316972/316972 [00:01<00:00, 233289.98it/s]\n",
      "100%|██████████| 376971/376971 [00:01<00:00, 237019.96it/s]\n"
     ]
    }
   ],
   "source": [
    "apper = 1\n",
    "material = pro_df['material'].tolist()\n",
    "material = [str(x) for x in material]\n",
    "material_key = list(set(material))\n",
    "material_number = len(material_key)\n",
    "material_values = list(range(1,len(material_key)+1))\n",
    "material_s2n = dict(zip(material_key, material_values))\n",
    "material_s2n['nan'] = 0\n",
    "material_map = [{} for i in range(temp)]\n",
    "material_locale = pro_df['locale'].tolist()\n",
    "material_item = pro_df['item'].tolist()\n",
    "\n",
    "\n",
    "for i in range(len(material)):\n",
    "    if material_locale[i] not in ['DE','UK','JP']:\n",
    "        continue\n",
    "    material_map[locale_map[material_locale[i]]][material_item[i]]= material_s2n[material[i]]\n",
    "\n",
    "material_window = [[ Counter() for _ in range(material_number+2)] for i in range(temp)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_material_window(material_window,row,locale__,1)\n",
    "        \n",
    "\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_material_window(material_window,row,locale__,1)\n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_material_window(material_window,row,locale__,2)\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_material_window(material_window,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaeba61-27ae-4d48-865c-099ce0bd4938",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:04<00:00, 658440.60it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 601512.56it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 734051.25it/s]\n",
      "100%|██████████| 3256352/3256352 [00:04<00:00, 754893.04it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 702369.94it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 796616.30it/s] \n",
      "100%|██████████| 3256352/3256352 [00:05<00:00, 645986.23it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 625090.76it/s]\n",
      "100%|██████████| 316972/316972 [00:00<00:00, 675130.07it/s] \n"
     ]
    }
   ],
   "source": [
    "for locale_ in ['DE','JP','UK']:\n",
    "    new_user = []\n",
    "    material_ratio = []\n",
    "    material_num = []\n",
    "    material_item = []\n",
    "    locale__ = locale_map[locale_]\n",
    "    for i in tqdm(range(len(train_session))):\n",
    "        \n",
    "        if train_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(train_user[i])\n",
    "        row = train_session[i][:-1]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if material_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        material_item.append(len(c))\n",
    "        material_num.append(len(set(c)))\n",
    "        material_ratio.append(len(c)/len(row))\n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        if valid_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(valid_user[i])\n",
    "        row = valid_session[i][:-1]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if material_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        material_item.append(len(c))\n",
    "        material_num.append(len(set(c)))\n",
    "        material_ratio.append(len(c)/len(row))\n",
    "        \n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        if test_locale[i] != locale_:\n",
    "            continue\n",
    "        new_user.append(test_user[i])\n",
    "        row = test_session[i]\n",
    "        a = 0\n",
    "        b = 0\n",
    "        c = []\n",
    "        for j in row:\n",
    "            if material_map[locale__][j] !=0:\n",
    "                c.append(j)\n",
    "        material_item.append(len(c))\n",
    "        material_num.append(len(set(c)))\n",
    "        material_ratio.append(len(c)/len(row))\n",
    "    user_pop = pd.DataFrame({'user':new_user,\n",
    "                             'material_item':material_item,\n",
    "                             'material_num':material_num,\n",
    "                             'material_ratio':material_ratio,\n",
    "                            }).set_index([\"user\"])\n",
    "    user_pop.to_parquet('user_material'+locale_+'.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee36e8a8-b50b-486f-bc91-e20b1d72e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "material_window_max = [[ 0 for _ in range(material_number+1)] for i in range(3)]\n",
    "brand_window_max = [[ 0 for _ in range(brand_number+1)] for i in range(3)]\n",
    "author_window_max = [[ 0 for _ in range(author_number+1)] for i in range(3)]\n",
    "for i in range(3):\n",
    "    for j in range(1,brand_number+1):\n",
    "        brand_window_max[i][j] = sum(brand_window[i][j].values())\n",
    "    for j in range(1,material_number+1):\n",
    "        material_window_max[i][j] = sum(material_window[i][j].values())\n",
    "    for j in range(author_number+1):\n",
    "        author_window_max[i][j] = sum(author_window[i][j].values())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "97c882fc-9393-443b-afc9-e96fce47be2d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 14.next item Multi-hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc012c-4734-4c5d-84a5-5b9cbeba8b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next(cov,row,locale__,apper,_max=None):\n",
    "    for j in range(len(row)-1):\n",
    "        cov[locale__][row[j]][row[j+1]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a966f1d1-c3e1-4c8f-89e1-0bf181b8f89c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:17<00:00, 191217.95it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 187860.52it/s]\n",
      "100%|██████████| 316972/316972 [00:01<00:00, 264489.20it/s]\n",
      "100%|██████████| 376971/376971 [00:01<00:00, 274267.78it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next_ = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    if i % 10 == 0 and data_type == 'train':\n",
    "        row = row[:-1]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next(item_next_,row,locale__,1)\n",
    "    \n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        row = valid_session[i]\n",
    "        locale__ = locale_map[valid_locale[i]]\n",
    "        make_item_next(item_next_,row,locale__,1)\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_item_next(item_next_,row,locale__,2)\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_item_next(item_next_,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a47d21-a2c8-449e-af19-dc89aa6eb53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410676/1410676 [02:18<00:00, 10205.89it/s]  \n",
      "100%|██████████| 1410676/1410676 [02:49<00:00, 8346.94it/s] \n",
      "100%|██████████| 1410676/1410676 [02:49<00:00, 8299.45it/s]   \n"
     ]
    }
   ],
   "source": [
    "item_next_double = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in range(3):\n",
    "    for j in tqdm(range(itemnum)):\n",
    "        for key in item_next_[i][j].keys():\n",
    "            item_next_double[i][j] += item_next_[i][j]\n",
    "            sum_ = 0\n",
    "            for key2 in item_next_[i][key].keys():\n",
    "                sum_ += item_next_[i][key][key2]\n",
    "            if sum_ == 0:\n",
    "                continue\n",
    "            for key2 in item_next_[i][key].keys():\n",
    "                if key2 in item_next_[i][j].keys():\n",
    "                    continue\n",
    "                item_next_double[i][j][key2] += (item_next_[i][key][key2]/sum_*item_next_[i][key][key2])/20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b652043b-5321-42ef-9c4d-80589a227d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next_double_max = get_max(item_next_double,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2202226-2602-41bc-8dad-110fa8c32050",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 15.last item Multi-hop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9e88c1-3917-4a20-9613-cf5478fb91f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next_last(cov,row,locale__,apper,_max=None): \n",
    "    for j in range(len(row)-1):\n",
    "        cov[locale__][row[j]][row[-1]] += apper\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb00c04d-9598-4fef-b399-9caedfd00fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:07<00:00, 430366.83it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 411881.92it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next_last_ = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    if i % 10 == 0 and data_type == 'train':\n",
    "        continue\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next_last(item_next_last_,row[-2:],locale__,1)\n",
    "if data_type =='test':\n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        row = valid_session[i]\n",
    "        locale__ = locale_map[valid_locale[i]]\n",
    "        make_item_next_last(item_next_last_,row[-2:],locale__,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facdbf6b-f828-4d70-bd2c-4c32cebdf2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1410676/1410676 [00:13<00:00, 101853.10it/s]\n",
      "100%|██████████| 1410676/1410676 [00:11<00:00, 122543.85it/s]\n",
      "100%|██████████| 1410676/1410676 [00:12<00:00, 116553.24it/s] \n"
     ]
    }
   ],
   "source": [
    "item_next_last_0_2_double = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in range(3):\n",
    "    for j in tqdm(range(itemnum)):\n",
    "        for key in item_next_last_[i][j].keys():\n",
    "            item_next_last_0_2_double[i][j] += item_next_last_[i][j]\n",
    "            sum_ = 0\n",
    "            for key2 in item_next_last_[i][key].keys():\n",
    "                sum_ += item_next_last_[i][key][key2]\n",
    "            if sum_ == 0:\n",
    "                continue\n",
    "            for key2 in item_next_last_[i][key].keys():\n",
    "                if key2 in item_next_last_[i][j].keys():\n",
    "                    continue\n",
    "                item_next_last_0_2_double[i][j][key2] += (item_next_last_[i][key][key2]/sum_*item_next_last_[i][key][key2])/10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa0e687-36b5-4b65-a099-05e7ef89bbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next_last_0_2_double_max = get_max(item_next_last_0_2_double,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4d16cf0-d6bf-499d-bad8-04cd353f35b3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 18.last back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09bcb4b-f1b1-4785-af7e-72f4c44c4596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_next_last_back(cov,row,locale__,apper,_max=None): \n",
    "    cov[locale__][row[-1]][row[-2]] += apper\n",
    "    if _max!=None:\n",
    "        _max[locale__][row[-1]] += apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2a1a88-1f30-4900-a0de-d317d3f7adec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:05<00:00, 578962.36it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 482628.69it/s]\n"
     ]
    }
   ],
   "source": [
    "item_next_last_back = [[ Counter() for _ in range(itemnum)] for i in range(temp)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_next_last_back(item_next_last_back,row,locale__,1)\n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_next_last_back(item_next_last_back,row,locale__,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94e55e-d1e6-43cc-9836-0cb598fcfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_next_last_back_max = get_max(item_next_last_back,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ec578e2f-cb4a-4dbd-a58b-c4d7727d0060",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 23 item mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afc3f9e-6969-4d6b-95d2-25dbfb9855cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_item_mix(cov,row,locale__,apper,_max=None):\n",
    "    for j in range(len(row)-1):\n",
    "        cov[locale__][row[j]][row[j+1]] += apper\n",
    "        cov[locale__][row[j+1]][row[j]] += apper*0.5\n",
    "        if _max!=None:\n",
    "            _max[locale__][row[j]] += apper\n",
    "            _max[locale__][row[j+1]] += apper*0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c090f-e1eb-4e0b-bfeb-c8aedaa7fa92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:24<00:00, 132685.40it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 122645.90it/s]\n",
      "100%|██████████| 316972/316972 [00:01<00:00, 183125.41it/s]\n",
      "100%|██████████| 376971/376971 [00:02<00:00, 186277.12it/s]\n"
     ]
    }
   ],
   "source": [
    "item_mix = [[ Counter() for _ in range(itemnum)] for i in range(3)]\n",
    "for i in tqdm(range(len(train_session))):\n",
    "    row = train_session[i]\n",
    "    locale__ = locale_map[train_locale[i]]\n",
    "    make_item_mix(item_mix,row,locale__,1)\n",
    "        \n",
    "for i in tqdm(range(len(valid_session))):\n",
    "    row = valid_session[i]\n",
    "    locale__ = locale_map[valid_locale[i]]\n",
    "    make_item_mix(item_mix,row,locale__,1)\n",
    "    \n",
    "if data_type == 'test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        locale__ = locale_map[test_locale[i]]\n",
    "        make_item_mix(item_mix,row,locale__,3)\n",
    "\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        locale__ = locale_map[test_locale2[i]]\n",
    "        make_item_mix(item_mix,row,locale__,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8484580-977b-44be-8a98-919bb7a9e69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mix_max = get_max(item_mix,itemnum)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be8f1c19-9ddc-45fa-9e0a-3ae8f8f0da5c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## pop feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770c3327-4415-4913-b0df-2e48ba296a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 316972/316972 [00:00<00:00, 435765.06it/s]\n",
      "100%|██████████| 376971/376971 [00:00<00:00, 420447.02it/s]\n"
     ]
    }
   ],
   "source": [
    "apper = 1\n",
    "item_sum_pop = [Counter() for _ in range(len(locale_map))]\n",
    "if data_type !='test':\n",
    "    for i in tqdm(range(len(train_session))):\n",
    "        row = train_session[i]\n",
    "        if i %10==0 and data_type == 'train':\n",
    "            row = row[:-1]\n",
    "        for item in set(row):\n",
    "            item_sum_pop[locale_map[train_locale[i]]][item]+= apper\n",
    "if data_type=='test':\n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        row = test_session[i]\n",
    "        for item in set(row):\n",
    "            item_sum_pop[locale_map[test_locale[i]]][item]= apper\n",
    "    for i in tqdm(range(len(test_session2))):\n",
    "        row = test_session2[i]\n",
    "        for item in set(row):\n",
    "            item_sum_pop[locale_map[test_locale2[i]]][item]= apper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69812ce6-6a6e-4e77-a4d9-56c6af864b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518327/518327 [00:00<00:00, 1181950.69it/s]\n",
      "100%|██████████| 395009/395009 [00:00<00:00, 1198095.69it/s]\n",
      "100%|██████████| 500180/500180 [00:00<00:00, 1224518.17it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tqdm import tqdm\n",
    "for locale_ in ['DE','JP','UK']:\n",
    "    sum_pop_score = []\n",
    "    item = []\n",
    "    temp_df = pro_df[pro_df['locale']==locale_]\n",
    "    temp_item = temp_df['item'].tolist()\n",
    "    for i in tqdm(range(len(temp_item))):\n",
    "        item.append(temp_item[i])\n",
    "        sum_pop_score.append(item_sum_pop[locale_map[locale_]][temp_item[i]])\n",
    "    pop_df = pd.DataFrame({'item':item,\n",
    "                           'sum_pop_score':sum_pop_score,\n",
    "                          }).set_index([\"item\"])\n",
    "    pop_df['sum_pop_score'] = pop_df['sum_pop_score'].astype('int32')\n",
    "    pop_df.to_parquet(data_type+'_item_pop'+locale_+'.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "223eb949-5db6-43b6-b6ef-9a0fa6881fc1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## count feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b482f05-bd50-40f4-9f84-654979edecb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3256352/3256352 [00:02<00:00, 1169709.87it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 1021408.55it/s]\n",
      "100%|██████████| 316971/316971 [00:00<00:00, 1370233.06it/s]\n",
      "100%|██████████| 3256352/3256352 [00:02<00:00, 1230473.49it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 1052563.96it/s]\n",
      "100%|██████████| 316971/316971 [00:00<00:00, 1212168.74it/s]\n",
      "100%|██████████| 3256352/3256352 [00:02<00:00, 1247835.20it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 1090995.07it/s]\n",
      "100%|██████████| 316971/316971 [00:00<00:00, 1367466.42it/s]\n",
      "100%|██████████| 3256352/3256352 [00:02<00:00, 1289394.48it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 1134641.36it/s]\n",
      "100%|██████████| 316971/316971 [00:00<00:00, 1352152.39it/s]\n",
      "100%|██████████| 3256352/3256352 [00:02<00:00, 1143930.06it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 1040879.45it/s]\n",
      "100%|██████████| 316971/316971 [00:00<00:00, 1301187.52it/s]\n",
      "100%|██████████| 3256352/3256352 [00:02<00:00, 1189082.85it/s]\n",
      "100%|██████████| 16364/16364 [00:00<00:00, 1051209.81it/s]\n",
      "100%|██████████| 316971/316971 [00:00<00:00, 1215815.84it/s]\n"
     ]
    }
   ],
   "source": [
    "for locale_ in ['DE','JP','UK']:\n",
    "    new_user = []\n",
    "    new_item = []\n",
    "    for i in tqdm(range(len(train_session))):\n",
    "        if train_locale[i] != locale_:\n",
    "            continue\n",
    "        for item in train_session[i]:\n",
    "            new_user.append(train_user[i])\n",
    "            new_item.append(item)\n",
    "            \n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        if valid_locale[i] != locale_:\n",
    "            continue\n",
    "        for item in valid_session[i]:\n",
    "            new_user.append(valid_user[i])\n",
    "            new_item.append(item)\n",
    "    \n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        if test_locale[i] != locale_:\n",
    "            continue\n",
    "        for item in test_session[i]:\n",
    "            new_user.append(test_user[i])\n",
    "            new_item.append(item)\n",
    "            \n",
    "    \n",
    "    df = pd.DataFrame({'user':new_user,'item':new_item}, dtype='int32')\n",
    "    \n",
    "    item_features = df.groupby('item').agg({'item':'count','user':'nunique'})\n",
    "    item_features.columns = ['item_item_count','item_user_count']\n",
    "    item_features['item_item_count'] = item_features['item_item_count'].astype('int16')\n",
    "    item_features['item_user_count'] = item_features['item_user_count'].astype('int16')\n",
    "    # item_features.to_parquet('item_count'+locale_+'.pqt')\n",
    "    \n",
    "    new_user = []\n",
    "    new_item = []\n",
    "    for i in tqdm(range(len(train_session))):\n",
    "        if train_locale[i] != locale_:\n",
    "            continue\n",
    "        for item in train_session[i][:-1]:\n",
    "            new_user.append(train_user[i])\n",
    "            new_item.append(item)\n",
    "            \n",
    "    for i in tqdm(range(len(valid_session))):\n",
    "        if valid_locale[i] != locale_:\n",
    "            continue\n",
    "        for item in valid_session[i][:-1]:\n",
    "            new_user.append(valid_user[i])\n",
    "            new_item.append(item)\n",
    "            \n",
    "    for i in tqdm(range(len(test_session))):\n",
    "        if test_locale[i] != locale_:\n",
    "            continue\n",
    "        for item in test_session[i]:\n",
    "            new_user.append(test_user[i])\n",
    "            new_item.append(item)\n",
    "    df = pd.DataFrame({'user':new_user,'item':new_item}, dtype='int32')\n",
    "    \n",
    "    user_features = df.groupby('user').agg({'user':'count','item':'nunique'}, dtype='int32')\n",
    "    user_features.columns = ['user_user_count','user_item_count']\n",
    "    user_features['user_user_count'] = user_features['user_user_count'] .astype('int16')\n",
    "    user_features['user_item_count'] = user_features['user_item_count'] .astype('int16')\n",
    "    user_features['remember_ratio'] = user_features['user_item_count']/user_features['user_user_count']\n",
    "    user_features['remember_ratio'] = user_features['remember_ratio'].astype('float32')\n",
    "    user_features.to_parquet('user_count'+locale_+'.pqt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9fdf6221",
   "metadata": {},
   "source": [
    "# make feature file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e9f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def process_train(df, recall, \n",
    "                  real_logits=None,\n",
    "                  name='train', data_type='train',batch=0,back=False,use_apper=False):\n",
    "    ret_df = []\n",
    "    sum_len = len(df)\n",
    "    recall_num = 250\n",
    "    if True:\n",
    "        if batch == 1:#iten next\n",
    "            tcov = item_next\n",
    "            tcov_max = item_next_max\n",
    "        elif batch == 2:#item next skip2\n",
    "            tcov = item_next2\n",
    "            tcov_max = item_next2_max\n",
    "        elif batch == 3:#item next skip3\n",
    "            tcov = item_next3\n",
    "            tcov_max = item_next3_max\n",
    "        elif batch == 4:#item back\n",
    "            tcov = item_next_back\n",
    "            tcov_max = item_next_back_max\n",
    "        elif batch == 5:#item back skip2\n",
    "            tcov = item_next2_back\n",
    "            tcov_max = item_next2_back_max\n",
    "        elif batch == 6: #last02\n",
    "            tcov = item_next_last_0_2\n",
    "            tcov_max = item_next_last_0_2_max\n",
    "        elif batch == 7: #last23\n",
    "            tcov = item_next_last_2_3\n",
    "            tcov_max = item_next_last_2_3_max\n",
    "        elif batch == 9: #last3~\n",
    "            tcov = item_next_last_5_5\n",
    "            tcov_max = item_next_last_5_5_max\n",
    "        elif batch == 10:#lasttime\n",
    "            tcov = item_last_time\n",
    "            tcov_max = item_last_time_max\n",
    "        elif batch == 11:#string\n",
    "            _=1\n",
    "        elif batch == 12:#cfmodel\n",
    "            _=2\n",
    "        elif batch == 13:#unique_pair\n",
    "            tcov = unique_pair\n",
    "            tcov_max = unique_pair_max\n",
    "        elif batch == 15:#brand_window\n",
    "            tcov = brand_window\n",
    "            tcov_max = brand_window_max\n",
    "        elif batch == 16:#author_window\n",
    "            tcov = author_window\n",
    "            tcov_max = author_window_max\n",
    "        elif batch == 17:#material_window\n",
    "            tcov = material_window\n",
    "            tcov_max = material_window_max\n",
    "        elif batch == 18:#brand\n",
    "            _=1\n",
    "        elif batch == 19:#author\n",
    "            _=1\n",
    "        elif batch == 20:#material\n",
    "            _=1\n",
    "        elif batch == 21:#next item Multi-hop\n",
    "            tcov = item_next_double\n",
    "            tcov_max = item_next_double_max\n",
    "        elif batch == 22:#last item Multi-hop\n",
    "            tcov = item_next_last_0_2_double\n",
    "            tcov_max = item_next_last_0_2_double_max\n",
    "        elif batch == 23:#lastback\n",
    "            tcov = item_next_last_back\n",
    "            tcov_max = item_next_last_back_max\n",
    "        elif batch == 27:#item mix\n",
    "            tcov = item_mix\n",
    "            tcov_max = item_mix_max\n",
    "    if batch == 0:\n",
    "        feat_num = [0,0]\n",
    "    else:\n",
    "        if back == False:\n",
    "            feat_num = [(batch-1)*7+1,(batch)*7]\n",
    "        else:\n",
    "            feat_num = [(batch-1)*7+1+700,(batch)*7+700]\n",
    "    number = 0\n",
    "    for locale_ in ['DE','JP','UK']:#\n",
    "        locale__ = locale_map[locale_]\n",
    "        temp_df = df[df['locale'] == locale_]\n",
    "        session = temp_df['session'].tolist()\n",
    "        user = temp_df['user'].tolist()\n",
    "        index = temp_df.index\n",
    "        \n",
    "        if batch == 0:\n",
    "            new_user = []\n",
    "            new_item = []\n",
    "            target = []\n",
    "            item_pop = pd.read_parquet(data_type+'_item_pop'+locale_+'.pqt')\n",
    "            user_count = pd.read_parquet('user_count'+locale_+'.pqt')\n",
    "            user_brand = pd.read_parquet('user_brand'+locale_+'.pqt')\n",
    "            user_author = pd.read_parquet('user_author'+locale_+'.pqt')\n",
    "            user_material = pd.read_parquet('user_material'+locale_+'.pqt')\n",
    "            item_price = pd.read_parquet('item_price'+locale_+'.pqt')\n",
    "            user_price = pd.read_parquet('user_price'+locale_+'.pqt')\n",
    "            rank = []\n",
    "            value = []\n",
    "            coarse = []\n",
    "            nn_logits = []\n",
    "        elif batch == 11:\n",
    "            string1 = temp_df['string1'].tolist()\n",
    "            string3 = temp_df['string3'].tolist()\n",
    "            ui_feat1 = []#-1score\n",
    "            ui_feat2 = []#-2score\n",
    "            ui_feat3 = []#-3score\n",
    "            ui_feat4 = []#5avg score\n",
    "            ui_feat5 = []#avg score\n",
    "            ui_feat6 = []#weight score\n",
    "            ui_feat7 = []#max\n",
    "        else:\n",
    "            ui_feat1 = []#-1score\n",
    "            ui_feat2 = []#-2score\n",
    "            ui_feat3 = []#-3score\n",
    "            ui_feat4 = []#5avg score\n",
    "            ui_feat5 = []#avg score\n",
    "            ui_feat6 = []#weight score\n",
    "            ui_feat7 = []#max\n",
    "        for i in tqdm(range(len(session))):\n",
    "            if data_type != 'test':\n",
    "                label = session[i][-1]\n",
    "            row = session[i]\n",
    "            if data_type != 'test':\n",
    "                prev_items = session[i][:-1]\n",
    "            else:\n",
    "                prev_items = session[i]\n",
    "            prev_items = list(dict.fromkeys(prev_items[::-1]))[::-1]\n",
    "            pad_none = -1\n",
    "            apper = -1\n",
    "            if use_apper==True:\n",
    "                if batch == 1:\n",
    "                    make_item_next(item_next,row,locale__,apper,item_next_max)\n",
    "                elif batch == 2:\n",
    "                    make_item_next2(item_next2,row,locale__,apper,item_next2_max)\n",
    "                elif batch == 3:\n",
    "                    make_item_next3(item_next3,row,locale__,apper,item_next3_max)\n",
    "                elif batch == 4:\n",
    "                    make_item_next_back(item_next_back,row,locale__,apper,item_next_back_max)\n",
    "                elif batch == 5:\n",
    "                    make_item_next2_back(item_next2_back,row,locale__,apper,item_next2_back_max)\n",
    "                elif batch == 6:\n",
    "                    make_item_next_last(item_next_last_0_2,row[-2:],locale__,apper,item_next_last_0_2_max)\n",
    "                elif batch == 7:\n",
    "                    make_item_next_last(item_next_last_2_3,row[-3:-2] + [row[-1]],locale__,apper,item_next_last_2_3_max)\n",
    "                elif batch == 9:\n",
    "                    make_item_next_last(item_next_last_5_5,row[:-3]+ [row[-1]],locale__,apper,item_next_last_5_5_max)\n",
    "                elif batch == 10:\n",
    "                    make_item_last_time(item_last_time,row,locale__,apper,item_last_time_max)\n",
    "                elif batch == 11:\n",
    "                    check1[string1[i]][label]+=apper\n",
    "                    check3[string3[i]][label]+=apper\n",
    "                elif batch == 13:\n",
    "                    make_unique_pair(unique_pair,row,locale__,apper,unique_pair_max)\n",
    "                elif batch == 15:\n",
    "                    make_brand_window(brand_window,row,locale__,apper,brand_window_max)\n",
    "                elif batch == 16:\n",
    "                    make_author_window(author_window,row,locale__,apper,author_window_max)\n",
    "                elif batch == 17:\n",
    "                    make_material_window(material_window,row,locale__,apper,material_window_max)\n",
    "                elif batch == 23:\n",
    "                    make_item_next_last_back(item_next_last_back,row,locale__,apper,item_next_last_back_max)\n",
    "                elif batch == 27:\n",
    "                    make_item_mix(item_mix,row,locale__,apper,item_mix_max)\n",
    "            for j in range(len(recall[index[i]])):\n",
    "                number += 1\n",
    "                recall_item = recall[index[i]][j]\n",
    "                ##############################################featur make#####################################################################\n",
    "                if batch==0:\n",
    "                    if data_type != 'test':\n",
    "                        if recall_item == label:\n",
    "                            target.append(1)\n",
    "                        else:\n",
    "                            target.append(0)\n",
    "                    new_user.append(user[i])\n",
    "                    new_item.append(recall_item)\n",
    "                    nn_logits.append(real_logits[index[i]][j])\n",
    "                elif batch == 11:\n",
    "                    if counter1[string1[i]] !=0:\n",
    "                        ui_feat1.append(check1[string1[i]][recall_item]/max(1,sum(check1[string1[i]].values())))\n",
    "                    else:\n",
    "                        ui_feat1.append(0)\n",
    "                    if counter3[string3[i]] !=0:\n",
    "                        ui_feat2.append(check3[string3[i]][recall_item]/max(1,sum(check3[string3[i]].values())))\n",
    "                    else:\n",
    "                        ui_feat2.append(0)\n",
    "                    ui_feat3.append(0)\n",
    "                    ui_feat4.append(0)\n",
    "                    ui_feat5.append(0)\n",
    "                    ui_feat6.append(0)\n",
    "                    ui_feat7.append(0)\n",
    "                elif batch == 12:\n",
    "                    ui_feat1.append(item_sim[locale__][prev_items[-1]][recall_item])\n",
    "                    \n",
    "                    if len(prev_items)>=2:\n",
    "                        ui_feat2.append(item_sim[locale__][prev_items[-2]][recall_item])\n",
    "                    else:\n",
    "                        ui_feat2.append(pad_none)\n",
    "                        \n",
    "                    if len(prev_items)>=3:\n",
    "                        ui_feat3.append(item_sim[locale__][prev_items[-3]][recall_item])\n",
    "                    else:\n",
    "                        ui_feat3.append(pad_none)\n",
    "                        \n",
    "                    temp_score = 0.\n",
    "                    temp_score = 0.\n",
    "                    for k in range(1,min(6,len(prev_items)+1)):\n",
    "                        t_score = item_sim[locale__][prev_items[-k]][recall_item]\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat4.append(temp_score/min(5,len(prev_items)))\n",
    "                    \n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        t_score = item_sim[locale__][prev_items[-k]][recall_item]\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat5.append(temp_score/len(prev_items))\n",
    "                    \n",
    "                    temp_score = 0.\n",
    "                    weight = 1.\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        if k >10:\n",
    "                            break\n",
    "                        elif k <=1:\n",
    "                            weight = 1\n",
    "                        elif k<=2:\n",
    "                            weight = 1/20\n",
    "                        elif k<=3:\n",
    "                            weight = 1/400\n",
    "                        elif k<=5:\n",
    "                            weight = 1/8000\n",
    "                        t_score = item_sim[locale__][prev_items[-k]][recall_item]\n",
    "                        temp_score += sqrt(t_score)*weight\n",
    "                    ui_feat6.append(temp_score)\n",
    "                    \n",
    "                    max_score = 0\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        max_score = max(max_score,item_sim[locale__][prev_items[-k]][recall_item])\n",
    "                    ui_feat7.append(max_score)\n",
    "                elif batch == 15:\n",
    "                    a = brand_map[locale__][prev_items[-1]]\n",
    "                    b = brand_map[locale__][recall_item]\n",
    "                    ui_feat1.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    if len(prev_items)>=2:\n",
    "                        a = brand_map[locale__][prev_items[-2]]\n",
    "                        ui_feat2.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    else:\n",
    "                        ui_feat2.append(pad_none)\n",
    "                    if len(prev_items)>=3:\n",
    "                        a = brand_map[locale__][prev_items[-3]]\n",
    "                        ui_feat3.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    else:\n",
    "                        ui_feat3.append(pad_none)\n",
    "                    temp_score = 0.\n",
    "                    for k in range(1,min(6,len(prev_items)+1)):\n",
    "                        a = brand_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat4.append(temp_score/min(5,len(prev_items)))\n",
    "                    \n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        a = brand_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat5.append(temp_score/len(prev_items))\n",
    "                    \n",
    "                    temp_score = 0.\n",
    "                    weight = 1.\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        if k >10:\n",
    "                            break\n",
    "                        elif k <=1:\n",
    "                            weight = 1\n",
    "                        elif k<=2:\n",
    "                            weight = 1/20\n",
    "                        elif k<=3:\n",
    "                            weight = 1/400\n",
    "                        elif k<=5:\n",
    "                            weight = 1/8000\n",
    "                        a = brand_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)*weight\n",
    "                    ui_feat6.append(temp_score)\n",
    "                    \n",
    "                    max_score = 0\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        a = brand_map[locale__][prev_items[-k]]\n",
    "                        max_score = max(max_score,tcov[locale__][a][b]/max(1,tcov_max[locale__][a]))\n",
    "                    ui_feat7.append(max_score)\n",
    "                elif batch == 16:\n",
    "                    a = author_map[locale__][prev_items[-1]]\n",
    "                    b = author_map[locale__][recall_item]\n",
    "                    ui_feat1.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    if len(prev_items)>=2:\n",
    "                        a = author_map[locale__][prev_items[-2]]\n",
    "                        ui_feat2.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    else:\n",
    "                        ui_feat2.append(pad_none)\n",
    "                    if len(prev_items)>=3:\n",
    "                        a = author_map[locale__][prev_items[-3]]\n",
    "                        ui_feat3.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    else:\n",
    "                        ui_feat3.append(pad_none)\n",
    "                    temp_score = 0.\n",
    "                    for k in range(1,min(6,len(prev_items)+1)):\n",
    "                        a = author_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat4.append(temp_score/min(5,len(prev_items)))\n",
    "                    \n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        a = author_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat5.append(temp_score/len(prev_items))\n",
    "                    \n",
    "                    temp_score = 0.\n",
    "                    weight = 1.\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        if k >10:\n",
    "                            break\n",
    "                        elif k <=1:\n",
    "                            weight = 1\n",
    "                        elif k<=2:\n",
    "                            weight = 1/20\n",
    "                        elif k<=3:\n",
    "                            weight = 1/400\n",
    "                        elif k<=5:\n",
    "                            weight = 1/8000\n",
    "                        a = author_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)*weight\n",
    "                    ui_feat6.append(temp_score)\n",
    "                    \n",
    "                    max_score = 0\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        a = author_map[locale__][prev_items[-k]]\n",
    "                        max_score = max(max_score,tcov[locale__][a][b]/max(1,tcov_max[locale__][a]))\n",
    "                    ui_feat7.append(max_score)\n",
    "                elif batch == 17:\n",
    "                    a = material_map[locale__][prev_items[-1]]\n",
    "                    b = material_map[locale__][recall_item]\n",
    "                    ui_feat1.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    if len(prev_items)>=2:\n",
    "                        a = material_map[locale__][prev_items[-2]]\n",
    "                        ui_feat2.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    else:\n",
    "                        ui_feat2.append(pad_none)\n",
    "                    if len(prev_items)>=3:\n",
    "                        a = material_map[locale__][prev_items[-3]]\n",
    "                        ui_feat3.append(sqrt(tcov[locale__][a][b]/max(1,tcov_max[locale__][a])))\n",
    "                    else:\n",
    "                        ui_feat3.append(pad_none)\n",
    "                    temp_score = 0.\n",
    "                    for k in range(1,min(6,len(prev_items)+1)):\n",
    "                        a = material_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat4.append(temp_score/min(5,len(prev_items)))\n",
    "                    \n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        a = material_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)\n",
    "                    ui_feat5.append(temp_score/len(prev_items))\n",
    "                    \n",
    "                    temp_score = 0.\n",
    "                    weight = 1.\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        if k >10:\n",
    "                            break\n",
    "                        elif k <=1:\n",
    "                            weight = 1\n",
    "                        elif k<=2:\n",
    "                            weight = 1/20\n",
    "                        elif k<=3:\n",
    "                            weight = 1/400\n",
    "                        elif k<=5:\n",
    "                            weight = 1/8000\n",
    "                        a = material_map[locale__][prev_items[-k]]\n",
    "                        t_score = tcov[locale__][a][b]/max(1,tcov_max[locale__][a])\n",
    "                        temp_score += sqrt(t_score)*weight\n",
    "                    ui_feat6.append(temp_score)\n",
    "                    \n",
    "                    max_score = 0\n",
    "                    for k in range(1,len(prev_items)+1):\n",
    "                        a = material_map[locale__][prev_items[-k]]\n",
    "                        max_score = max(max_score,tcov[locale__][a][b]/max(1,tcov_max[locale__][a]))\n",
    "                    ui_feat7.append(max_score)\n",
    "                elif batch == 18:\n",
    "                    if brand_map[locale__][recall_item]==0:\n",
    "                        ui_feat1.append(-1)\n",
    "                        ui_feat2.append(-1)\n",
    "                        ui_feat3.append(-1)\n",
    "                        ui_feat4.append(-1)\n",
    "                        ui_feat5.append(-1)\n",
    "                        ui_feat6.append(-1)\n",
    "                        ui_feat7.append(-1)\n",
    "                    else:\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if brand_map[locale__][recall_item] == brand_map[locale__][prev_items[-k]] and brand_map[locale__][recall_item]*brand_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat1.append(sqrt(temp_score/len(prev_items)))\n",
    "                        if brand_map[locale__][recall_item] == brand_map[locale__][prev_items[-1]] and brand_map[locale__][recall_item]*brand_map[locale__][prev_items[-1]]!=0:\n",
    "                            ui_feat2.append(1)\n",
    "                        else:\n",
    "                            ui_feat2.append(0)\n",
    "\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(4,len(prev_items)+1)):\n",
    "                            if brand_map[locale__][recall_item] == brand_map[locale__][prev_items[-k]] and brand_map[locale__][recall_item]*brand_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat3.append(sqrt(temp_score/min(3,len(prev_items))))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(6,len(prev_items)+1)):\n",
    "                            if brand_map[locale__][recall_item] == brand_map[locale__][prev_items[-k]] and brand_map[locale__][recall_item]*brand_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat4.append(sqrt(temp_score/min(5,len(prev_items))))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if brand_map[locale__][recall_item] == brand_map[locale__][prev_items[-k]] and brand_map[locale__][recall_item]*brand_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat5.append(sqrt(temp_score/len(prev_items)))\n",
    "                        temp_score = 0.\n",
    "                        weight = 1.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if k >10:\n",
    "                                break\n",
    "                            elif k <=1:\n",
    "                                weight = 1\n",
    "                            elif k<=2:\n",
    "                                weight = 1/20\n",
    "                            elif k<=3:\n",
    "                                weight = 1/400\n",
    "                            elif k<=5:\n",
    "                                weight = 1/8000\n",
    "                            if brand_map[locale__][recall_item] == brand_map[locale__][prev_items[-k]] and brand_map[locale__][recall_item]*brand_map[locale__][prev_items[-k]]!=0:\n",
    "                                t_score = 1\n",
    "                            else:\n",
    "                                t_score = 0\n",
    "                            temp_score += t_score*weight\n",
    "                        ui_feat6.append(temp_score)\n",
    "                        ui_feat7.append(-1)\n",
    "                elif batch == 19:\n",
    "                    if author_map[locale__][recall_item]==0:\n",
    "                        ui_feat1.append(-1)\n",
    "                        ui_feat2.append(-1)\n",
    "                        ui_feat3.append(-1)\n",
    "                        ui_feat4.append(-1)\n",
    "                        ui_feat5.append(-1)\n",
    "                        ui_feat6.append(-1)\n",
    "                        ui_feat7.append(-1)\n",
    "                    else:\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if author_map[locale__][recall_item] == author_map[locale__][prev_items[-k]] and author_map[locale__][recall_item]*author_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat1.append(sqrt(temp_score/len(prev_items)))\n",
    "                        if author_map[locale__][recall_item] == author_map[locale__][prev_items[-1]] and author_map[locale__][recall_item]*author_map[locale__][prev_items[-1]]!=0:\n",
    "                            ui_feat2.append(1)\n",
    "                        else:\n",
    "                            ui_feat2.append(0)\n",
    "\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(4,len(prev_items)+1)):\n",
    "                            if author_map[locale__][recall_item] == author_map[locale__][prev_items[-k]] and author_map[locale__][recall_item]*author_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat3.append(sqrt(temp_score/min(3,len(prev_items))))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(6,len(prev_items)+1)):\n",
    "                            if author_map[locale__][recall_item] == author_map[locale__][prev_items[-k]] and author_map[locale__][recall_item]*author_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat4.append(sqrt(temp_score/min(5,len(prev_items))))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if author_map[locale__][recall_item] == author_map[locale__][prev_items[-k]] and author_map[locale__][recall_item]*author_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat5.append(sqrt(temp_score/len(prev_items)))\n",
    "                        temp_score = 0.\n",
    "                        weight = 1.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if k >10:\n",
    "                                break\n",
    "                            elif k <=1:\n",
    "                                weight = 1\n",
    "                            elif k<=2:\n",
    "                                weight = 1/20\n",
    "                            elif k<=3:\n",
    "                                weight = 1/400\n",
    "                            elif k<=5:\n",
    "                                weight = 1/8000\n",
    "                            if author_map[locale__][recall_item] == author_map[locale__][prev_items[-k]] and author_map[locale__][recall_item]*author_map[locale__][prev_items[-k]]!=0:\n",
    "                                t_score = 1\n",
    "                            else:\n",
    "                                t_score = 0\n",
    "                            temp_score += t_score*weight\n",
    "                        ui_feat6.append(temp_score)\n",
    "                        ui_feat7.append(temp_score)\n",
    "                elif batch == 20:\n",
    "                    if material_map[locale__][recall_item]==0:\n",
    "                        ui_feat1.append(-1)\n",
    "                        ui_feat2.append(-1)\n",
    "                        ui_feat3.append(-1)\n",
    "                        ui_feat4.append(-1)\n",
    "                        ui_feat5.append(-1)\n",
    "                        ui_feat6.append(-1)\n",
    "                        ui_feat7.append(-1)\n",
    "                    else:\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if material_map[locale__][recall_item] == material_map[locale__][prev_items[-k]] and material_map[locale__][recall_item]*material_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat1.append(sqrt(temp_score/len(prev_items)))\n",
    "                        if material_map[locale__][recall_item] == material_map[locale__][prev_items[-1]] and material_map[locale__][recall_item]*material_map[locale__][prev_items[-1]]!=0:\n",
    "                            ui_feat2.append(1)\n",
    "                        else:\n",
    "                            ui_feat2.append(0)\n",
    "\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(4,len(prev_items)+1)):\n",
    "                            if material_map[locale__][recall_item] == material_map[locale__][prev_items[-k]] and material_map[locale__][recall_item]*material_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat3.append(sqrt(temp_score/min(3,len(prev_items))))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(6,len(prev_items)+1)):\n",
    "                            if material_map[locale__][recall_item] == material_map[locale__][prev_items[-k]] and material_map[locale__][recall_item]*material_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat4.append(sqrt(temp_score/min(5,len(prev_items))))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if material_map[locale__][recall_item] == material_map[locale__][prev_items[-k]] and material_map[locale__][recall_item]*material_map[locale__][prev_items[-k]]!=0:\n",
    "                                temp_score+=1\n",
    "                        ui_feat5.append(sqrt(temp_score/len(prev_items)))\n",
    "                        temp_score = 0.\n",
    "                        weight = 1.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if k >10:\n",
    "                                break\n",
    "                            elif k <=1:\n",
    "                                weight = 1\n",
    "                            elif k<=2:\n",
    "                                weight = 1/20\n",
    "                            elif k<=3:\n",
    "                                weight = 1/400\n",
    "                            elif k<=5:\n",
    "                                weight = 1/8000\n",
    "                            if material_map[locale__][recall_item] == material_map[locale__][prev_items[-k]] and material_map[locale__][recall_item]*material_map[locale__][prev_items[-k]]!=0:\n",
    "                                t_score = 1\n",
    "                            else:\n",
    "                                t_score = 0\n",
    "                            temp_score += t_score*weight\n",
    "                        ui_feat6.append(temp_score)\n",
    "                        ui_feat7.append(-1)\n",
    "                elif batch == 50:\n",
    "                    x = prev_items[-1]\n",
    "                    y = recall_item\n",
    "                    score = 0\n",
    "                    user_set = set(item_user[locale__][x]) & set(item_user[locale__][y])\n",
    "                    for ua in user_set:\n",
    "                        for ub in user_set:\n",
    "                            if ua == ub:\n",
    "                                continue\n",
    "                            ua_session = session_all[ua]\n",
    "                            ub_session = session_all[ub]\n",
    "                            score += 1/np.sqrt(len(set(ua_session))*len(set(ub_session)))/(1+len(set(ua_session)&set(ub_session)))\n",
    "                    ui_feat1.append(score)\n",
    "                    ui_feat2.append(0)\n",
    "                    ui_feat3.append(0)\n",
    "                    ui_feat4.append(0)\n",
    "                    ui_feat5.append(0)\n",
    "                    ui_feat6.append(0)\n",
    "                    ui_feat7.append(0)\n",
    "                else:\n",
    "                    if back == False:\n",
    "                        #普通1\n",
    "                        min_sum = 1\n",
    "                        ui_feat1.append(sqrt(max(0,tcov[locale__][prev_items[-1]][recall_item])/max(min_sum,tcov_max[locale__][prev_items[-1]])))\n",
    "                        if len(prev_items)>=2:\n",
    "                            ui_feat2.append(sqrt(max(0,tcov[locale__][prev_items[-2]][recall_item])/max(min_sum,tcov_max[locale__][prev_items[-2]])))\n",
    "                        else:\n",
    "                            ui_feat2.append(pad_none)\n",
    "                        if len(prev_items)>=3:\n",
    "                            ui_feat3.append(sqrt(max(0,tcov[locale__][prev_items[-3]][recall_item])/max(min_sum,tcov_max[locale__][prev_items[-3]])))\n",
    "                        else:\n",
    "                            ui_feat3.append(pad_none)\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(6,len(prev_items)+1)):\n",
    "                            t_score = max(0,tcov[locale__][prev_items[-k]][recall_item])/max(min_sum,tcov_max[locale__][prev_items[-k]])\n",
    "                            temp_score += sqrt(t_score)\n",
    "                        ui_feat4.append(temp_score/min(5,len(prev_items)))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            t_score = max(0,tcov[locale__][prev_items[-k]][recall_item])/max(min_sum,tcov_max[locale__][prev_items[-k]])\n",
    "                            temp_score += sqrt(t_score)\n",
    "                        ui_feat5.append(temp_score/len(prev_items))\n",
    "                        temp_score = 0.\n",
    "                        weight = 1.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if k >10:\n",
    "                                break\n",
    "                            elif k <=1:\n",
    "                                weight = 1\n",
    "                            elif k<=2:\n",
    "                                weight = 1/20\n",
    "                            elif k<=3:\n",
    "                                weight = 1/400\n",
    "                            elif k<=5:\n",
    "                                weight = 1/8000\n",
    "                            t_score = max(0,tcov[locale__][prev_items[-k]][recall_item])/max(min_sum,tcov_max[locale__][prev_items[-k]])\n",
    "                            temp_score += sqrt(t_score)*weight\n",
    "                        ui_feat6.append(temp_score)\n",
    "                        max_score = 0\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            max_score = max(max_score,max(0,tcov[locale__][prev_items[-k]][recall_item])/max(min_sum,tcov_max[locale__][prev_items[-k]]))\n",
    "                        ui_feat7.append(max_score)\n",
    "                    else:\n",
    "                        #普通1\n",
    "                        ui_feat1.append(sqrt(max(0,tcov[locale__][recall_item][prev_items[-1]])/max(min_sum,tcov_max[locale__][recall_item])))\n",
    "                        if len(prev_items)>=2:\n",
    "                            ui_feat2.append(sqrt(max(0,tcov[locale__][recall_item][prev_items[-2]])/max(min_sum,tcov_max[locale__][recall_item])))\n",
    "                        else:\n",
    "                            ui_feat2.append(pad_none)\n",
    "                        if len(prev_items)>=3:\n",
    "                            ui_feat3.append(sqrt(max(0,tcov[locale__][recall_item][prev_items[-3]])/max(min_sum,tcov_max[locale__][recall_item])))\n",
    "                        else:\n",
    "                            ui_feat3.append(pad_none)\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,min(6,len(prev_items)+1)):\n",
    "                            t_score = max(0,tcov[locale__][recall_item][prev_items[-k]])/max(min_sum,tcov_max[locale__][recall_item])\n",
    "                            temp_score += sqrt(t_score)\n",
    "                        ui_feat4.append(temp_score/min(5,len(prev_items)))\n",
    "                        temp_score = 0.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            t_score = max(0,tcov[locale__][recall_item][prev_items[-k]])/max(min_sum,tcov_max[locale__][recall_item])\n",
    "                            temp_score += sqrt(t_score)\n",
    "                        ui_feat5.append(temp_score/len(prev_items))\n",
    "                        temp_score = 0.\n",
    "                        weight = 1.\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            if k >10:\n",
    "                                break\n",
    "                            elif k <=1:\n",
    "                                weight = 1\n",
    "                            elif k<=2:\n",
    "                                weight = 1/20\n",
    "                            elif k<=3:\n",
    "                                weight = 1/400\n",
    "                            elif k<=5:\n",
    "                                weight = 1/8000\n",
    "                            t_score = max(0,tcov[locale__][recall_item][prev_items[-k]])/max(min_sum,tcov_max[locale__][recall_item])\n",
    "                            temp_score += sqrt(t_score)*weight\n",
    "                        ui_feat6.append(temp_score)\n",
    "                        max_score = 0\n",
    "                        for k in range(1,len(prev_items)+1):\n",
    "                            max_score = max(max_score,max(0,tcov[locale__][recall_item][prev_items[-k]])/max(min_sum,tcov_max[locale__][recall_item]))\n",
    "                        ui_feat7.append(max_score)\n",
    "                ##################################################################################################################        \n",
    "            apper = 1\n",
    "            if use_apper==True:\n",
    "                if batch == 1:\n",
    "                    make_item_next(item_next,row,locale__,apper,item_next_max)\n",
    "                elif batch == 2:\n",
    "                    make_item_next2(item_next2,row,locale__,apper,item_next2_max)\n",
    "                elif batch == 3:\n",
    "                    make_item_next3(item_next3,row,locale__,apper,item_next3_max)\n",
    "                elif batch == 4:\n",
    "                    make_item_next_back(item_next_back,row,locale__,apper,item_next_back_max)\n",
    "                elif batch == 5:\n",
    "                    make_item_next2_back(item_next2_back,row,locale__,apper,item_next2_back_max)\n",
    "                elif batch == 6:\n",
    "                    make_item_next_last(item_next_last_0_2,row[-2:],locale__,apper,item_next_last_0_2_max)\n",
    "                elif batch == 7:\n",
    "                    make_item_next_last(item_next_last_2_3,row[-3:-2] + [row[-1]],locale__,apper,item_next_last_2_3_max)\n",
    "                elif batch == 9:\n",
    "                    make_item_next_last(item_next_last_5_5,row[:-3]+ [row[-1]],locale__,apper,item_next_last_5_5_max)\n",
    "                elif batch == 10:\n",
    "                    make_item_last_time(item_last_time,row,locale__,apper,item_last_time_max)\n",
    "                elif batch == 11:\n",
    "                    check1[string1[i]][label]+=apper\n",
    "                    check3[string3[i]][label]+=apper\n",
    "                elif batch == 13:\n",
    "                    make_unique_pair(unique_pair,row,locale__,apper,unique_pair_max)\n",
    "                elif batch == 15:\n",
    "                    make_brand_window(brand_window,row,locale__,apper,brand_window_max)\n",
    "                elif batch == 16:\n",
    "                    make_author_window(author_window,row,locale__,apper,author_window_max)\n",
    "                elif batch == 17:\n",
    "                    make_material_window(material_window,row,locale__,apper,material_window_max)\n",
    "                elif batch == 23:\n",
    "                    make_item_next_last_back(item_next_last_back,row,locale__,apper,item_next_last_back_max)\n",
    "                elif batch == 27:\n",
    "                    make_item_mix(item_mix,row,locale__,apper,item_mix_max)\n",
    "            \n",
    "        if number!=0:\n",
    "            number = 0\n",
    "            output = pd.DataFrame()\n",
    "            if batch == 0:\n",
    "                output = pd.DataFrame({'user':new_user,'item':new_item}, dtype='int32')\n",
    "                output = output.merge(user_count, left_on='user', right_index=True, how='left').fillna(-1)\n",
    "                output = output.merge(item_pop, left_on='item', right_index=True, how='left').fillna(-1)\n",
    "                output = output.merge(item_price, left_on='item', right_index=True, how='left').fillna(-1)\n",
    "                output = output.merge(user_price, left_on='user', right_index=True, how='left').fillna(-1)\n",
    "                output = output.merge(user_brand, left_on='user', right_index=True, how='left').fillna(-1)\n",
    "                output = output.merge(user_author, left_on='user', right_index=True, how='left').fillna(-1)\n",
    "                output = output.merge(user_material, left_on='user', right_index=True, how='left').fillna(-1)\n",
    "                output['nn_logits'] = pd.DataFrame(nn_logits, dtype='float32')\n",
    "                output['pop'] = np.sqrt(output['sum_pop_score']/max(output['sum_pop_score']))\n",
    "                if data_type != 'test':\n",
    "                    output['target'] = pd.DataFrame(target, dtype='int8')\n",
    "            else:\n",
    "                output['ui_feat'+str(0+feat_num[0])] = pd.DataFrame(ui_feat1,dtype='float32')\n",
    "                output['ui_feat'+str(1+feat_num[0])] = pd.DataFrame(ui_feat2,dtype='float32')\n",
    "                output['ui_feat'+str(2+feat_num[0])] = pd.DataFrame(ui_feat3,dtype='float32')\n",
    "                output['ui_feat'+str(3+feat_num[0])] = pd.DataFrame(ui_feat4,dtype='float32')\n",
    "                output['ui_feat'+str(4+feat_num[0])] = pd.DataFrame(ui_feat5,dtype='float32')\n",
    "                output['ui_feat'+str(5+feat_num[0])] = pd.DataFrame(ui_feat6,dtype='float32')\n",
    "                output['ui_feat'+str(6+feat_num[0])] = pd.DataFrame(ui_feat7,dtype='float32')\n",
    "            ret_df.append(output)\n",
    "            ret_df = pd.concat(ret_df).reset_index(drop=True)\n",
    "            ret_df.to_parquet(name+'_data_'+locale_+'_'+str(feat_num[0])+'~'+str(feat_num[1])+'.pqt')\n",
    "            ret_df = []\n",
    "            # return\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdb4954",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == 'train':\n",
    "    train_recall_data =  pd.read_pickle('./recall_train_250_with_nn_p2.dataset')\n",
    "    # new_logits = pd.read_pickle('./logits_train_2.dataset')\n",
    "    for i in [0,1,2,3,4,5,6,7,9,10,11,12,13,16,15,17,18,19,21,22,23,27]:\n",
    "        process_train(train_data.iloc[list(range(0,len(train_data),10))].reset_index(drop=True),\n",
    "                    train_recall_data['next_item_prediction'].tolist(),\n",
    "                    train_recall_data['nn_logits'].tolist(),\n",
    "                    data_type='train',\n",
    "                    name='train5',\n",
    "                    batch=i,#This parameter indicates which matrix to use to build the feature.The numbers here are not the numbers in markdown when the matrix was built above.\n",
    "                    use_apper=True,\n",
    "                    # back=True,\n",
    "                    )\n",
    "    del train_recall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4e75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == 'train':\n",
    "    valid_recall_data =  pd.read_pickle('./recall_valid_250_with_nn_p2.dataset')\n",
    "    # new_logits = pd.read_pickle('./logits_valid_2.dataset')\n",
    "    for i in [0,1,2,3,4,5,6,7,9,10,11,12,13,16,15,17,18,19,21,22,23,27]:\n",
    "        process_train(valid_data,\n",
    "                    valid_recall_data['next_item_prediction'].tolist(),\n",
    "                    valid_recall_data['nn_logits'].tolist(),\n",
    "                    data_type='train',\n",
    "                    name='valid5',\n",
    "                    batch=i,\n",
    "                    use_apper=True,\n",
    "                    # back=True,\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07d7f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_type == 'test':\n",
    "    test_recall_data =  pd.read_pickle('./recall_test_250_with_nn_p2.dataset')\n",
    "    # new_logits = pd.read_pickle('./logits_test_2.dataset')\n",
    "    for i in [0,1,2,3,4,5,6,7,9,10,11,12,13,16,15,17,18,19,21,22,23,27]:\n",
    "        process_train(test_data,\n",
    "                    test_recall_data['next_item_prediction'].tolist(),\n",
    "                    test_recall_data['nn_logits'].tolist(),\n",
    "                    data_type='test',\n",
    "                    name='test5',\n",
    "                    batch=i,\n",
    "                    use_apper=False,\n",
    "                    )\n",
    "    del test_recall_data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b631a683-b9c0-432b-aeb9-21b8c6685be4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0697dd-e823-4371-9d4a-03bf9f8134da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "330046a8-6c51-47bd-80ce-467886fdc99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "itemnum = 1410675 + 1\n",
    "#locale map number\n",
    "locale_map = {}\n",
    "temp = 0\n",
    "for item in list(set(train_locale)):\n",
    "    locale_map[item] = temp\n",
    "    temp += 1\n",
    "id2index = [{} for _ in range(temp)]\n",
    "item = pro_df['item'].tolist()\n",
    "item_locale = pro_df['locale'].tolist()\n",
    "for i in range(len(item)):\n",
    "    if item_locale[i] not in ['DE','UK','JP']:\n",
    "        continue\n",
    "    locale_ = locale_map[item_locale[i]]\n",
    "    id2index[locale_][item[i]] = i"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5044b54-b790-4064-aed9-4443ba37bfe7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ec4622-2235-4347-9245-3e8f35f34a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def process_train(df, recall, text_embedding, name='train',batch=0):\n",
    "    ret_df = []\n",
    "    sum_len = len(df)\n",
    "    recall_num = 250\n",
    "    apper = 1\n",
    "    feat_num = [200,206]\n",
    "    number = 0\n",
    "    for locale_ in ['DE','JP','UK']:#\n",
    "        locale__ = locale_map[locale_]\n",
    "        temp_df = df[df['locale'] == locale_]\n",
    "        session = temp_df['session'].tolist()\n",
    "        user = temp_df['user'].tolist()\n",
    "        index = temp_df.index\n",
    "        ui_feat1 = []#-1 sim\n",
    "        ui_feat2 = []#-2 sim\n",
    "        ui_feat3 = []#-3 sim\n",
    "        ui_feat4 = []#-5 avg sim\n",
    "        ui_feat5 = []#avg sim\n",
    "        ui_feat6 = []#weight sim\n",
    "        ui_feat7 = []#max\n",
    "\n",
    "        for i in tqdm(range(len(session))):\n",
    "            row = session[i]\n",
    "            temp_list = [1,2,3,5,20]\n",
    "            prev_items = session[i]\n",
    "            if name[:4] != 'test':\n",
    "                prev_items = prev_items[:-1]\n",
    "            prev_items = list(dict.fromkeys(prev_items[::-1]))\n",
    "            with autocast():\n",
    "                prev_embedding = text_embedding[torch.tensor([id2index[locale__][x] for x in prev_items]).to(device),:]\n",
    "                recall_embedding = text_embedding[torch.tensor([id2index[locale__][x] for x in recall[index[i]]]).to(device),:]\n",
    "                sim = recall_embedding.mm(prev_embedding.T).cpu()\n",
    "            \n",
    "            for j in range(sim.shape[0]):\n",
    "                number += 1\n",
    "                \n",
    "                ##############################################featur make#####################################################################\n",
    "                ui_feat1.append(sim[j][0].cpu().item())\n",
    "                if sim.shape[1]>=2:\n",
    "                    ui_feat2.append(sim[j][1].cpu().item())\n",
    "                else:\n",
    "                    ui_feat2.append(0)\n",
    "                if sim.shape[1]>=3:\n",
    "                    ui_feat3.append(sim[j][2].cpu().item())\n",
    "                else:\n",
    "                    ui_feat3.append(0)\n",
    "                ui_feat4.append(sim[j][:5].mean(dim=0).cpu().item())\n",
    "                ui_feat5.append(sim[j].mean(dim=0).cpu().item())\n",
    "                temp_score = 0.\n",
    "                weight = 1.\n",
    "                for k in range(0,sim.shape[1]):\n",
    "                    if k >10:\n",
    "                        break\n",
    "                    elif k <1:\n",
    "                        weight = 1\n",
    "                    elif k<2:\n",
    "                        weight = 1/20\n",
    "                    elif k<3:\n",
    "                        weight = 1/400\n",
    "                    elif k<5:\n",
    "                        weight = 1/8000\n",
    "                    temp_score += sim[j][k].cpu().item()*weight\n",
    "                ui_feat6.append(temp_score)\n",
    "                ui_feat7.append(sim[j].max(0).values.item())\n",
    "                ##################################################################################################################        \n",
    "\n",
    "        output = pd.DataFrame()\n",
    "        output['ui_feat200'] = pd.DataFrame(ui_feat1,dtype='float32')\n",
    "        output['ui_feat201'] = pd.DataFrame(ui_feat2,dtype='float32')\n",
    "        output['ui_feat202'] = pd.DataFrame(ui_feat3,dtype='float32')\n",
    "        output['ui_feat203'] = pd.DataFrame(ui_feat4,dtype='float32')\n",
    "        output['ui_feat204'] = pd.DataFrame(ui_feat5,dtype='float32')\n",
    "        output['ui_feat205'] = pd.DataFrame(ui_feat6,dtype='float32')\n",
    "        output['ui_feat206'] = pd.DataFrame(ui_feat7,dtype='float32')\n",
    "        output.to_parquet(name+'_data_'+locale_+'_'+str(feat_num[0])+'~'+str(feat_num[1])+'.pqt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14efe3d-a925-4010-8ee1-b1ac541c556c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 110498/110498 [29:20<00:00, 62.78it/s]\n",
      "100%|██████████| 97168/97168 [25:40<00:00, 63.07it/s]\n",
      " 97%|█████████▋| 114456/117970 [29:52<00:57, 61.20it/s]"
     ]
    }
   ],
   "source": [
    "title_embedding = torch.load('../data/title_embeddings_128.dataset').to(device)\n",
    "title_embedding = F.normalize(title_embedding, p=2, dim=1)\n",
    "train_recall_data =  pd.read_pickle('./recall_train_250_with_nn.dataset')['next_item_prediction'].tolist()\n",
    "process_train(train_data.iloc[list(range(0,len(train_data),10))].reset_index(drop=True),\n",
    "          train_recall_data,\n",
    "          title_embedding,\n",
    "          batch=0,\n",
    "            name='train5'\n",
    "         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5024659-70a4-4ae3-a0c4-3713e456bb8d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## valid&test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130c82ff-190f-495b-bc34-797ae8052a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_recall_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d1ced44-e7d2-4ac5-ad93-8a1fd570f43c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5632/5632 [01:29<00:00, 63.08it/s]\n",
      "100%|██████████| 4894/4894 [01:16<00:00, 63.67it/s]\n",
      "100%|██████████| 5838/5838 [01:31<00:00, 63.81it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_recall_data =  pd.read_pickle('./recall_valid_250_with_nn.dataset')['next_item_prediction'].tolist()\n",
    "process_train(valid_data,\n",
    "              valid_recall_data,\n",
    "              title_embedding.to(device),\n",
    "              batch=0,\n",
    "              name='valid5'\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b542ef0b-5afa-44d9-848e-129c48d8ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 104568/104568 [28:34<00:00, 60.99it/s]\n",
      "100%|██████████| 115936/115936 [39:58<00:00, 48.34it/s]\n"
     ]
    }
   ],
   "source": [
    "test_recall_data =  pd.read_pickle('./recall_test_250_with_nn.dataset')['next_item_prediction'].tolist()\n",
    "process_train(test_data,\n",
    "              test_recall_data,\n",
    "              title_embedding.to(device),\n",
    "              batch=0,\n",
    "              name='test5'\n",
    "             )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
