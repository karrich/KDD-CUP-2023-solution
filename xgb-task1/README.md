# Feature generation
In the feature generation code, a large number of sparse matrices are built (we also treat item-cf as a matrix). 
For each matrix, we generate 7 features in the same way. 

Assuming that `P(,)` is the matrix, 
`y` represents the candidate item,
and `x` is the item in the session,
then the formula for constructing these 7 features is:
![image](https://github.com/karrich/KDD-CUP-2023-solution/assets/57396778/3f75a4bc-fff7-4170-ba1a-36b1c9cfac06)
![image](https://github.com/karrich/KDD-CUP-2023-solution/assets/57396778/e39e7482-8abe-450e-a29a-cb2535bc8dae)
![image](https://github.com/karrich/KDD-CUP-2023-solution/assets/57396778/d1d036d3-4d6d-41d9-a7f0-6c37064e7b50)
![image](https://github.com/karrich/KDD-CUP-2023-solution/assets/57396778/83e109d8-c1c5-42c5-b2d7-dac6dd8de082)
![image](https://github.com/karrich/KDD-CUP-2023-solution/assets/57396778/2fe4c6af-c98c-45c3-bef1-52a842d2e71a)
![image](https://github.com/karrich/KDD-CUP-2023-solution/assets/57396778/a4efedaf-3dcb-4bb7-9c90-249b9152750c)
![image](https://github.com/karrich/KDD-CUP-2023-solution/assets/57396778/b29655e0-f661-40ae-a5fc-0f5c27b3bd53)

To avoid feature leakage, we remove the effect of this session on the matrix when building features for each session-candidate pair. After building this feature for the session, add the effect of the session on the matrix. The reason why we don't remove the last column of data directly at training time is because we find the features built by the last column of data very useful, and our approach does not lead to unacceptable time complexity.
# Rerank model
The sorting model we use xgboost, and the specific parameters can be viewed in the code. 
We also tried catboost and LGBM, but catboost was a big mistake and maybe we didn't find the best hyperparameters. 
And we had some problems with LGBM and we were running out of time when we found them. 
So in the end, we only used a single xgboost model on task1 and task2.
# Trick
1. We found that when building a matrix, increasing the effect of the test set on the matrix can obtain better results.
2. We found that when a product only appears in the training set, but not in the test set, lowering its final score can significantly improve the effect!
# others
1. For task1, we only used 10% of the data for training. (Of course, we used all the data when building the matrix) but even then, more than 50GB of memory and about 30GB of GPU memory were required to train the model.
2. And because the number is still very large, it may not be possible for most computers to generate features with one click. In my code, I put the features generated by each matrix in the right file. So there are as many feature files as there are matrices (in fact, more, because there are other types of features). For computers with small memory, we recommend building one matrix at a time, then generating features before deleting this matrix and building the next matrix...
